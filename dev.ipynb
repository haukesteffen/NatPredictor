{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NATIONALITY PREDICTION**\n",
    "\n",
    "The goal of this notebook is to create a model that can predict nationalities from name strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import country_converter as coco\n",
    "from data_util import NameNationalityData, NameNationalityDataStream\n",
    "\n",
    "device: str = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "MAXIMUM_NAME_LENGTH: int = 50 # maximum number of characters\n",
    "BATCH_SIZE: int = 1024 # number of training examples per batch\n",
    "N_EVAL: int = 100 # evaluate loss every n batches\n",
    "N_TRAINING_STEPS: int = 10000 # number of trainings steps \n",
    "\n",
    "# read country codes\n",
    "with open('./data/.country_codes', 'r') as f:\n",
    "    COUNTRY_CODES: list = f.read().splitlines()\n",
    "\n",
    "#read vocabulary (all unique characters used in the dataset)\n",
    "with open('./data/.vocabulary', 'r') as f:\n",
    "    VOCABULARY: str = f.read()\n",
    "\n",
    "# generate country code mappings\n",
    "target_class: str = 'UNregion' # see country_converter documentation on PyPI for available classes\n",
    "COUNTRY_MAPPING: dict = {cc: coco.convert(names=cc, to=target_class) for cc in COUNTRY_CODES} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPORT DATA**\n",
    "\n",
    "- train.csv gets streamed in chunks\n",
    "- val.csv will be loaded into memory as a whole\n",
    "- name strings will be encoded as integer tensors where index i maps to the i-th character in the vocabulary\n",
    "- zero will be used as padding index, names longer than max_name_length will be truncated\n",
    "- the tensors will have a shape of (batch_size, max_name_length)\n",
    "- the dataset also generates a tensor of shape (batch_size) that holds the sequence length (number of characters) of the current name\n",
    "- countries will be converted to one-hot-encoded tensors of shape (batch_size, n_countries+1) where n_countries is the number of output classes in the COUNTRY_MAPPING dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NameNationalityDataStream(\n",
    "    data_file='./data/train.csv',\n",
    "    chunksize=100*BATCH_SIZE,\n",
    "    maximum_name_length=MAXIMUM_NAME_LENGTH,\n",
    "    vocabulary=VOCABULARY,\n",
    "    country_codes=COUNTRY_CODES,\n",
    "    country_mapping=COUNTRY_MAPPING\n",
    ")\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = NameNationalityData(\n",
    "    data_file='./data/val.csv',\n",
    "    maximum_name_length=MAXIMUM_NAME_LENGTH,\n",
    "    vocabulary=VOCABULARY,\n",
    "    country_codes=COUNTRY_CODES,\n",
    "    country_mapping=COUNTRY_MAPPING\n",
    ")\n",
    "val_dataloader = DataLoader(val_data, batch_size=N_EVAL*BATCH_SIZE, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MODELING**\n",
    "\n",
    "- Create simple model using character embeddings, rnn layers and a dense layer\n",
    "- embedding layer maps input tensor of shape (batch_size, max_name_length) to embedding tensor of shape (batch_size, max_name_length, embedding_dim)\n",
    "- the embedding tensor and sequence_lengths tensor will be used to [pack a padded batch](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html), which enables variable length inputs\n",
    "- the packed sequence will be passed to the rnn layer \n",
    "- the hidden state of the last rnn layer will be used passed through a dense layer to create an output of shape (batch_size, n_countries+1), where where n_countries is the number of output classes in the COUNTRY_MAPPING dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Nationality_Predictor(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch-based RNN model for predicting nationality from a name.\n",
    "\n",
    "    This model embeds input characters and processes them using a recurrent layer,\n",
    "    which can be instantiated as a vanilla RNN, GRU, or LSTM. It leverages sequence\n",
    "    packing to efficiently handle variable-length inputs, and uses the final hidden state\n",
    "    from the RNN to produce class logits through a dense layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    architecture : str\n",
    "        The type of RNN to use. Must be one of: 'RNN', 'GRU', or 'LSTM'.\n",
    "    embedding_dim : int\n",
    "        The dimension of the embedding space for input characters.\n",
    "    hidden_size : int\n",
    "        The number of features in the hidden state of the RNN.\n",
    "    num_rnn_layers : int\n",
    "        The number of recurrent layers (stacked) in the RNN.\n",
    "    dropout : float\n",
    "        Dropout probability applied between RNN layers.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    embed : nn.Embedding\n",
    "        The embedding layer that converts input indices to dense vectors.\n",
    "    rnn : nn.Module\n",
    "        The recurrent layer (RNN, GRU, or LSTM) that processes the embedded sequence.\n",
    "    dense : nn.Linear\n",
    "        A linear layer that maps the final hidden state to output logits corresponding\n",
    "        to the target nationality classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, architecture, embedding_dim, hidden_size, num_rnn_layers, dropout):\n",
    "        super().__init__()\n",
    "        # hyperparameters\n",
    "        self.architecture = architecture\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_rnn_layers = num_rnn_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # embedding layer\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=len(VOCABULARY)+1,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        # rnn layers\n",
    "        if architecture == 'RNN':\n",
    "            rnn_constructor = nn.RNN\n",
    "        elif architecture == 'GRU':\n",
    "            rnn_constructor = nn.GRU\n",
    "        elif architecture == 'LSTM':\n",
    "            rnn_constructor = nn.LSTM\n",
    "        else:\n",
    "            raise NameError(\"architecture must be 'RNN', 'GRU' or 'LSTM'\")\n",
    "        self.rnn = rnn_constructor(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_rnn_layers,\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        # dense layer\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=len(set(COUNTRY_MAPPING.values()))+1,\n",
    "        )\n",
    "\n",
    "    def forward(self, X, lengths):\n",
    "        embeddings = self.embed(X)\n",
    "\n",
    "        # Pack the padded batch\n",
    "        packed = pack_padded_sequence(\n",
    "            embeddings,\n",
    "            lengths=lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        if self.architecture == 'LSTM':\n",
    "            _, (hidden, _) = self.rnn(packed) # output and cell state ignored\n",
    "        else:\n",
    "            _, hidden = self.rnn(packed) # output ignored\n",
    "        logits = self.dense(hidden[-1])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_Nationality_Predictor(\n",
    "    architecture='LSTM',\n",
    "    embedding_dim=64,\n",
    "    hidden_size=128,\n",
    "    num_rnn_layers=3,\n",
    "    dropout=0.3\n",
    ")\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "criterion = F.binary_cross_entropy_with_logits\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        n_training_steps: int,\n",
    "        n_eval: int\n",
    "    ) -> None:\n",
    "    batch_number: int = 1\n",
    "    losses: list = []\n",
    "    val_losses: list = []\n",
    "\n",
    "    while True:\n",
    "        for X, y, sequence_lenghts in train_dataloader:\n",
    "            batch_number += 1\n",
    "            model.train()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X, sequence_lenghts)\n",
    "            loss = criterion(logits, y)\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                if batch_number%n_eval==0:\n",
    "                    model.eval()\n",
    "                    X, y, sequence_lenghts = next(iter(val_dataloader))\n",
    "                    X, y = X.to(device), y.to(device)\n",
    "                    logits = model(X, sequence_lenghts)\n",
    "                    val_loss = criterion(logits, y)\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    print(f'batch {batch_number} --- mean training loss over last {n_eval} batches: {np.mean(losses[-n_eval:]):.5f} --- validation loss: {val_loss:.5f}')\n",
    "            if batch_number >= n_training_steps:\n",
    "                return batch_number, losses, val_losses\n",
    "            \n",
    "batch_number, losses, val_losses = train(N_TRAINING_STEPS, N_EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test names for all 19 regions\n",
    "test_names = {\n",
    "    # Africa\n",
    "    \"Northern Africa\": \"Abdel Fattah el-Sisi\", # Egypt\n",
    "    \"Middle Africa\": \"João Lourenço\", # Angola\n",
    "    \"Western Africa\": \"Bola Ahmed Tinubu\", # Nigeria\n",
    "    \"Eastern Africa\": \"Taye Atske Selassie\", # Ethiopia\n",
    "    \"Southern Africa\": \"Cyril Ramaphosa\", # South Africa\n",
    "\n",
    "    # Asia\n",
    "    \"Central Asia\": \"Qassym-Schomart Kemeluly Toqajew\", # Kazakhstan\n",
    "    \"Eastern Asia\": \"Xi Jinping\", # China\n",
    "    \"South-Eastern Asia\": \"Prabowo Subianto\", # Indonesia\n",
    "    \"Southern Asia\": \"Droupadi Murmu\", # India\n",
    "    \"Western Asia\": \"Recep Tayyip Erdoğan\", # Turkey\n",
    "\n",
    "    # Europe\n",
    "    \"Northern Europe\": \"Ulf Kristersson\", # Sweden\n",
    "    \"Western Europe\": \"Olaf Scholz\", # Germany\n",
    "    \"Southern Europe\": \"Giorgia Meloni\", # Italy\n",
    "    \"Eastern Europe\": \"Andrzej Sebastian Duda\", # Poland\n",
    "\n",
    "    # Americas\n",
    "    \"Northern America\": \"Donald Trump\", # United States\n",
    "    \"Central America\": \"Andrés Manuel López Obrador\", # Mexico\n",
    "    \"Caribbean\": \"Andrew Holness\", # Jamaica\n",
    "    \"South America\": \"Luiz Inácio Lula da Silva\", # Brazil\n",
    "\n",
    "    # Oceania\n",
    "    \"Oceania\": \"Anthony Albanese\" # Australia\n",
    "}\n",
    "\n",
    "# run test on test names\n",
    "model.eval()\n",
    "tensor, length = train_data._encode_name(list(test_names.values()))\n",
    "tensor = tensor.to(device)\n",
    "logits = model(tensor, length)\n",
    "countries_list = train_data._decode_country(logits)\n",
    "preds = dict(zip(test_names.values(), countries_list))\n",
    "\n",
    "# define column widths\n",
    "name_width = 40\n",
    "actual_width = 20\n",
    "predicted_width = 20\n",
    "correct_width = 10\n",
    "\n",
    "# print output header\n",
    "header = f\"{'Name':<{name_width}} {'Actual Class':<{actual_width}} {'Predicted Class':<{predicted_width}} {'Correct?':<{correct_width}}\"\n",
    "print(header)\n",
    "print(\"-\" * (name_width + actual_width + predicted_width + correct_width))\n",
    "\n",
    "# loop through test names and format outputs\n",
    "total = 0\n",
    "correct_count = 0\n",
    "for actual_class, name in test_names.items():\n",
    "    predicted_class = preds.get(name, \"N/A\")\n",
    "    is_correct = predicted_class == actual_class\n",
    "    correct_str = \"Yes\" if is_correct else \"No\"\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    total += 1\n",
    "    row = f\"{name:<{name_width}} {actual_class:<{actual_width}} {predicted_class:<{predicted_width}} {correct_str:<{correct_width}}\"\n",
    "    print(row)\n",
    "accuracy = (correct_count / total) * 100\n",
    "print(f'\\nAccuracy: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
