{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NATIONALITY PREDICTION**\n",
    "\n",
    "The goal of this notebook is to create a model that can predict nationalities from name strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Country codes: MY, CR, AZ, TM, AL, BW, MX, MO, NA, TN, AO, BG, UY, ZA, BF, NG, BD, BR, BE, CA, LY, IR, IE, KZ, FJ, EG, ID, IS, HU, IQ, FI, EE, PS, QA, PE, PR, SI, ES, HT, JO, IT, GH, PA, DE, KH, EC, ET, SY, PT, HR, JM, IL, DK, DJ, KR, HK, SV, SA, PL, RS, GE, GR, IN, HN, DZ, FR, SD, PH, SE, JP, GB, SG, RU, GT, KW, LT, BH, CL, TR, CZ, AE, CM, BI, AR, LB, LU, MD, CO, AF, CY, CN, OM, MA, MV, BN, YE, BO, AT, NL, MU, US, TW, CH, MT, NO\n",
      "Vocabulary:  !#$%&()*-./:;<=ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}Â¡Â¢Â£Â¤Â¥Â¦Â§Â¨Â©ÂªÂ«Â¬Â®Â¯Â°Â±Â´ÂµÂ¶Â·Â¸ÂºÂ»Â¼Â½Â¾Â¿Ã€ÃÃ‚ÃƒÃ„Ã…Ã†Ã‡ÃˆÃ‰ÃŠÃ‹ÃŒÃÃŽÃÃÃ‘Ã’Ã“Ã”Ã•Ã–Ã—Ã˜Ã™ÃšÃ›ÃœÃÃžÃŸÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã¸Ã¹ÃºÃ»Ã¼Ã½Ã¾Ã¿Ä€ÄÄ‚ÄƒÄ„Ä…Ä†Ä‡ÄˆÄ‰ÄŠÄ‹ÄŒÄÄŽÄÄÄ‘Ä’Ä“Ä”Ä•Ä–Ä—Ä˜Ä™ÄšÄ›ÄœÄÄžÄŸÄ Ä¡Ä¢Ä£Ä¤Ä¥Ä¦Ä§Ä¨Ä©ÄªÄ«Ä¬Ä­Ä®Ä¯Ä°Ä±Ä²Ä³Ä´ÄµÄ¶Ä·Ä¸Ä¹ÄºÄ»Ä¼Ä½Ä¾Ä¿Å€ÅÅ‚ÅƒÅ„Å…Å†Å‡ÅˆÅ‰ÅŠÅ‹ÅŒÅÅŽÅÅÅ‘Å’Å“Å”Å•Å–Å—Å˜Å™ÅšÅ›ÅœÅÅžÅŸÅ Å¡Å¢Å£Å¤Å¥Å¦Å§Å¨Å©ÅªÅ«Å¬Å­Å®Å¯Å°Å±Å²Å³Å´ÅµÅ¶Å·Å¸Å¹ÅºÅ»Å¼Å½Å¾Å¿Æ€ÆÆ‚ÆƒÆ„Æ…Æ†Æ‡ÆˆÆ‰ÆŠÆ‹ÆŒÆÆŽÆÆÆ‘Æ’Æ“Æ”Æ•Æ–Æ—Æ˜Æ™ÆšÆ›ÆœÆÆžÆŸÆ Æ¡Æ¢Æ£Æ¤Æ¥Æ¦Æ§Æ¨Æ©ÆªÆ«Æ¬Æ­Æ®Æ¯Æ°Æ±Æ²Æ³Æ´ÆµÆ¶Æ¸Æ¹ÆºÆ»Æ¼Æ½Æ¾Æ¿Ç€Ç‚Ç…Ç†ÇÇŽÇÇÇ‘Ç’Ç“Ç”Ç•Ç–Ç—Ç˜Ç™ÇšÇ›ÇœÇÇžÇŸÇ Ç¡Ç¢Ç£Ç¤Ç¥Ç¦Ç§Ç¨Ç©ÇªÇ«Ç¬Ç­Ç®Ç¯Ç°Ç³Ç´ÇµÇ¶Ç·Ç¸Ç¹ÇºÇ»Ç¼Ç½Ç¾Ç¿È€ÈÈ‚ÈƒÈ„È…È†È‡ÈˆÈ‰ÈŠÈ‹ÈŒÈÈŽÈÈÈ‘È’È“È”È•È–È—È˜È™ÈšÈ›ÈœÈÈžÈŸÈ È¡È¢È£È¤È¥È¦È§È¨È©ÈªÈ«È¬È­È®È¯È°È±È²È³È´ÈµÈ¶È·È¸È¹ÈºÈ»È¼È½È¾È¿É€ÉƒÉ„É…É†É‡ÉˆÉ‰ÉŠÉ‹ÉŒÉÉŽÉÉÉ‘É’É“É”É•É–É—É˜É™ÉšÉ›ÉœÉÉžÉŸÉ É¡É¢É£É¤É¥É¦É§É¨É©ÉªÉ«É­É®É¯É±É²É³É´ÉµÉ¶É·É¸É¹ÉºÉ½É¾É¿Ê€ÊÊ‚ÊƒÊ„Ê…Ê†Ê‡ÊˆÊ‰ÊŠÊ‹ÊŒÊÊŽÊÊÊ‘Ê’Ê“Ê˜Ê™ÊšÊ›ÊœÊÊžÊŸÊ Ê¡Ê¢Ê¥Ê©Ê¬Ê­Ê®Ê¯Ê°Ê³Ê¹Ê¼Ê¾Ë†Ë‡ËŽËË˜ËšË›ËœËŸË¡Ë¢Ë£Ë«Ì€ÌÌ‚ÌƒÌ„Ì…Ì†Ì‡ÌˆÌ‰ÌŠÌ‹ÌŒÌÌŽÌÌÌ‘Ì’Ì“Ì”Ì•Ì–Ì—Ì˜Ì™ÌšÌ›ÌœÌÌžÌŸÌ Ì¡Ì¢Ì£Ì¤Ì¥Ì¦Ì§Ì¨Ì©ÌªÌ«Ì¬Ì­Ì®Ì¯Ì°Ì±Ì²Ì³Ì´ÌµÌ¶Ì·Ì¸Ì¹ÌºÌ»Ì¼Ì½Ì¾Ì¿Í‚Í…Í†Í‡ÍˆÍ‰ÍŠÍ‹ÍŒÍÍŽÍÍÍ‘Í’Í“Í”Í•Í–Í—Í˜Í™ÍšÍ›ÍœÍÍžÍŸÍ Í¡Í¢Í£Í¤Í¥Í¦Í§Í¨Í©ÍªÍ«Í¬Í­Í®Í¯Í¶Í·ÍºÍ¼Í½Í¿Î†ÎˆÎ‰ÎŠÎŒÎŽÎÎÎ‘Î’Î“Î”Î•Î–Î—Î˜Î™ÎšÎ›ÎœÎÎžÎŸÎ Î¡Î£Î¤Î¥Î¦Î§Î¨Î©ÎªÎ«Î¬Î­Î®Î¯Î°Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏ‚ÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰ÏŠÏ‹ÏŒÏÏŽÏÏÏ‘Ï’Ï“Ï”Ï•Ï–Ï—Ï˜Ï™ÏšÏ›ÏœÏÏ¢Ï¥Ï§Ï¨Ï©ÏªÏ«Ï­Ï¯Ï²Ï³ÏµÏºÏ»Ï¾Ï¿Ð€ÐÐ‚ÐƒÐ„Ð…Ð†Ð‡ÐˆÐ‰ÐŠÐ‹ÐŒÐÐŽÐÐÐ‘Ð’Ð“Ð”Ð•Ð–Ð—Ð˜Ð™ÐšÐ›ÐœÐÐžÐŸÐ Ð¡Ð¢Ð£Ð¤Ð¥Ð¦Ð§Ð¨Ð©ÐªÐ«Ð¬Ð­Ð®Ð¯Ð°Ð±Ð²Ð³Ð´ÐµÐ¶Ð·Ð¸Ð¹ÐºÐ»Ð¼Ð½Ð¾Ð¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑˆÑ‰ÑŠÑ‹ÑŒÑÑŽÑÑÑ‘Ñ’Ñ“Ñ”Ñ•Ñ–Ñ—Ñ˜Ñ™ÑšÑ›ÑœÑÑžÑŸÑ Ñ¡Ñ¢Ñ£Ñ¥Ñ¦Ñ§Ñ¨Ñ©ÑªÑ«Ñ¬Ñ­Ñ®Ñ¯Ñ°Ñ±Ñ²Ñ³Ñ´ÑµÑ¶Ñ·ÑºÑ»Ñ¼Ñ½Ñ¾Ñ¿ÒÒƒÒ„Ò…Ò†Ò‡ÒˆÒ‰ÒŠÒ‹ÒÒŽÒÒÒ‘Ò’Ò“Ò”Ò•Ò–Ò—Ò˜Ò™ÒšÒ›ÒœÒÒžÒŸÒ Ò¡Ò¢Ò£Ò¤Ò¥Ò¦Ò§Ò¨Ò©ÒªÒ«Ò¬Ò­Ò®Ò¯Ò°Ò±Ò²Ò³Ò´ÒµÒ¶Ò·Ò¸Ò¹ÒºÒ»Ò¼Ò½Ò¾Ò¿Ó€ÓÓ‚ÓƒÓ„Ó†Ó‡ÓˆÓ‰ÓŠÓÓŽÓÓÓ‘Ó’Ó“Ó•Ó—Ó˜Ó™ÓšÓ›ÓÓŸÓ Ó¡Ó¢Ó£Ó¤Ó¥Ó¦Ó§Ó¨Ó©ÓªÓ«Ó­Ó®Ó¯Ó±Ó²Ó³ÓµÓ¶Ó·Ó»Ó¼Ó½Ó¿Ô€ÔÔ‚ÔƒÔ„Ô…Ô†Ô‡ÔŠÔ‹ÔŒÔÔŽÔÔ‘Ô”Ô•Ô–Ô—ÔšÔ›Ô±Ô²Ô³Ô´ÔµÔ¶Ô·Ô¸Ô¹ÔºÔ»Ô¼Ô½Ô¾Ô¿Õ€ÕÕ‚ÕƒÕ„Õ…Õ†Õ‡ÕˆÕ‰ÕŠÕ‹ÕŒÕÕŽÕÕÕ‘Õ’Õ“Õ”Õ•Õ–Õ™ÕšÕœÕžÕ¡Õ¢Õ£Õ¤Õ¥Õ¦Õ§Õ¨Õ©ÕªÕ«Õ¬Õ­Õ®Õ¯Õ°Õ±Õ²Õ³Õ´ÕµÕ¶Õ·Õ¸Õ¹ÕºÕ»Õ¼Õ½Õ¾Õ¿Ö€ÖÖ‚ÖƒÖ„Ö…Ö†Ö‡Ö‰ÖŠÖ”Ö™ÖŸÖ«Ö®Ö°Ö±Ö²Ö³Ö´ÖµÖ¶Ö·Ö¸Ö¹ÖºÖ»Ö¼Ö¾Ö¿××‚×†×‡××‘×’×“×”×•×–×—×˜×™×š×›×œ××ž×Ÿ× ×¡×¢×£×¤×¥×¦×§×¨×©×ª×°×±×²×³×´ØŒØØØØ‘Ø’Ø“Ø”Ø•Ø–Ø˜Ø™ØšØ›ØžØŸØ Ø¡Ø¢Ø£Ø¤Ø¥Ø¦Ø§Ø¨Ø©ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºØ»Ø¼Ø½Ø¾Ø¿Ù€ÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙ‰ÙŠÙ‹ÙŒÙÙŽÙÙÙ‘Ù’Ù“Ù”Ù•Ù–Ù—Ù˜Ù™ÙšÙ›ÙœÙÙžÙªÙ«Ù¬Ù­Ù®Ù¯Ù°Ù±Ù²Ù³Ù´ÙµÙ¶Ù·Ù¸Ù¹ÙºÙ»Ù¼Ù½Ù¾Ù¿Ú€ÚÚ‚ÚƒÚ„Ú…Ú†Ú‡ÚˆÚ‰ÚŠÚ‹ÚŒÚÚŽÚÚÚ‘Ú’Ú“Ú”Ú•Ú–Ú—Ú˜Ú™ÚšÚ›ÚœÚÚžÚŸÚ Ú¡Ú¢Ú£Ú¤Ú¥Ú¦Ú§Ú¨Ú©ÚªÚ«Ú¬Ú­Ú®Ú¯Ú°Ú±Ú²Ú³Ú´ÚµÚ¶Ú·Ú¸Ú¹ÚºÚ»Ú¼Ú½Ú¾Ú¿Û€ÛÛ‚ÛƒÛ„Û…Û†Û‡ÛˆÛ‰ÛŠÛ‹ÛŒÛÛŽÛÛÛ‘Û’Û“Û”Û•Û–Û—Û˜Û™ÛšÛ›ÛœÛžÛŸÛ Û¡Û¢Û£Û¤Û¥Û¦Û§Û¨ÛªÛ«Û¬Û­Û®Û¯ÛºÛ»Û¼Û½Û¿Ü€ÜƒÜÜ‘Ü’Ü“Ü”Ü•Ü–Ü—Ü˜Ü™ÜšÜ›ÜœÜÜžÜŸÜ Ü¡Ü¢Ü£Ü¤Ü¥Ü¦Ü§Ü¨Ü©ÜªÜ«Ü¬Ü­Ü°Ü²Ü³ÜµÜ¶Ü¸Ü¹ÜºÜ¼Ü½Ü¿ÝÝ‚ÝƒÝ„Ý…Ý†Ý‡ÝˆÝ‰ÝŠÝÝ‘Ý’Ý“Ý”Ý•Ý–Ý—Ý˜Ý™ÝšÝ›ÝœÝÝžÝŸÝ Ý¡Ý¢Ý£Ý¤Ý¥Ý¦Ý§Ý¨Ý©ÝªÝ«Ý¬Ý­Ý¯Ý°Ý±Ý²Ý³Ý´Ý¶Ý·Ý¸Ý¹ÝºÝ»Ý¼Ý½Ý¾Ý¿Þ€ÞÞ‚ÞƒÞ„Þ…Þ†Þ‡ÞˆÞ‰ÞŠÞ‹ÞŒÞÞŽÞÞÞ‘Þ’Þ“Þ”Þ•Þ–Þ—Þ˜Þ™ÞšÞ›ÞœÞÞžÞŸÞ Þ¢Þ£Þ¤Þ¦Þ§Þ¨Þ©ÞªÞ«Þ¬Þ­Þ®Þ¯Þ°Þ±ßŠß‹ßŒßßŽßßß’ß“ß”ß•ß–ß—ß˜ß™ßšß›ßœßßžßŸß ß¡ß¢ß£ß¤ß¥ß¦ß«ß¬ß­ß®ß°ß±ß²ß´ßºà ƒà¡€à¡ƒà¡…à¡‰à¡‹à¡Œà¡Žà£¥à£¨à£©à£ªà£«à£°à£²à£³à£µà£·à£¸à¤à¤‚à¤ƒà¤…à¤†à¤‡à¤ˆà¤‰à¤Šà¤‹à¤Œà¤à¤Žà¤à¤à¤‘à¤’à¤“à¤”à¤•à¤–à¤—à¤˜à¤™à¤šà¤›à¤œà¤à¤žà¤Ÿà¤ à¤¡à¤¢à¤£à¤¤à¤¥à¤¦à¤§à¤¨à¤©à¤ªà¤«à¤¬à¤­à¤®à¤¯à¤°à¤±à¤²à¤³à¤´à¤µà¤¶à¤·à¤¸à¤¹à¤»à¤¼à¤½à¤¾à¤¿à¥€à¥à¥‚à¥ƒà¥„à¥…à¥†à¥‡à¥ˆà¥‰à¥Šà¥‹à¥Œà¥à¥à¥à¥‘à¥’à¥“à¥”à¥•à¥–à¥—à¥™à¥œà¥à¥ à¥¡à¥¢à¥£à¥¤à¥¥à¥°à¥±à¥²à¥¶à¥¼à¦à¦‚à¦ƒà¦…à¦†à¦‡à¦ˆà¦‰à¦Šà¦‹à¦Œà¦à¦à¦“à¦”à¦•à¦–à¦—à¦˜à¦™à¦šà¦›à¦œà¦à¦žà¦Ÿà¦ à¦¡à¦¢à¦£à¦¤à¦¥à¦¦à¦§à¦¨à¦ªà¦«à¦¬à¦­à¦®à¦¯à¦°à¦²à¦¶à¦·à¦¸à¦¹à¦¼à¦½à¦¾à¦¿à§€à§à§‚à§ƒà§„à§‡à§ˆà§‹à§Œà§à§Žà§—à§œà§Ÿà§ à§¡à§¢à§£à§°à§±à¨à¨‚à¨ƒà¨…à¨†à¨‡à¨ˆà¨‰à¨Šà¨à¨à¨“à¨”à¨•à¨–à¨—à¨˜à¨™à¨šà¨›à¨œà¨à¨žà¨Ÿà¨ à¨¡à¨¢à¨£à¨¤à¨¥à¨¦à¨§à¨¨à¨ªà¨«à¨¬à¨­à¨®à¨¯à¨°à¨²à¨µà¨¶à¨¸à¨¹à¨¼à¨¾à¨¿à©€à©à©‚à©‡à©ˆà©‹à©Œà©à©‘à©™à©›à©œà©°à©±à©²à©³à©´à©µàªàª‚àªƒàª…àª†àª‡àªˆàª‰àªŠàª‹àªŒàªàªàªàª‘àª“àª”àª•àª–àª—àª˜àª™àªšàª›àªœàªàªžàªŸàª àª¡àª¢àª£àª¤àª¥àª¦àª§àª¨àªªàª«àª¬àª­àª®àª¯àª°àª²àª³àªµàª¶àª·àª¸àª¹àª¼àª½àª¾àª¿à«€à«à«‚à«ƒà«„à«…à«‡à«ˆà«‰à«‹à«Œà«à«à« à«£à¬à¬‚à¬ƒà¬…à¬†à¬‡à¬ˆà¬‰à¬Šà¬‹à¬à¬à¬“à¬”à¬•à¬–à¬—à¬˜à¬™à¬šà¬›à¬œà¬à¬žà¬Ÿà¬ à¬¡à¬¢à¬£à¬¤à¬¥à¬¦à¬§à¬¨à¬ªà¬«à¬¬à¬­à¬®à¬¯à¬°à¬²à¬³à¬µà¬¶à¬·à¬¸à¬¹à¬¼à¬½à¬¾à¬¿à­€à­à­‚à­ƒà­„à­‡à­ˆà­‹à­Œà­à­Ÿà­±à®‚à®ƒà®…à®†à®‡à®ˆà®‰à®Šà®Žà®à®à®’à®“à®”à®•à®™à®šà®œà®žà®Ÿà®£à®¤à®¨à®©à®ªà®®à®¯à®°à®±à®²à®³à®´à®µà®¶à®·à®¸à®¹à®¾à®¿à¯€à¯à¯‚à¯†à¯‡à¯ˆà¯Šà¯‹à¯Œà¯à¯à¯—à°à°‚à°ƒà°…à°†à°‡à°ˆà°‰à°Šà°‹à°Žà°à°à°’à°“à°”à°•à°–à°—à°˜à°™à°šà°›à°œà°à°žà°Ÿà° à°¡à°¢à°£à°¤à°¥à°¦à°§à°¨à°ªà°«à°¬à°­à°®à°¯à°°à°±à°²à°³à°µà°¶à°·à°¸à°¹à°½à°¾à°¿à±€à±à±‚à±ƒà±„à±†à±‡à±ˆà±Šà±‹à±Œà±à±–à±™à± à±¢à²‚à²ƒà²…à²†à²‡à²ˆà²‰à²Šà²‹à²Œà²Žà²à²à²’à²“à²”à²•à²–à²—à²˜à²™à²šà²›à²œà²à²žà²Ÿà² à²¡à²¢à²£à²¤à²¥à²¦à²§à²¨à²ªà²«à²¬à²­à²®à²¯à²°à²±à²²à²³à²µà²¶à²·à²¸à²¹à²¼à²¾à²¿à³€à³à³‚à³ƒà³†à³‡à³ˆà³Šà³‹à³Œà³à³•à³–à³ à³¡à´‚à´ƒà´…à´†à´‡à´ˆà´‰à´Šà´‹à´Žà´à´à´’à´“à´”à´•à´–à´—à´˜à´™à´šà´›à´œà´à´žà´Ÿà´ à´¡à´¢à´£à´¤à´¥à´¦à´§à´¨à´ªà´«à´¬à´­à´®à´¯à´°à´±à´²à´³à´´à´µà´¶à´·à´¸à´¹à´ºà´½à´¾à´¿àµ€àµàµ‚àµƒàµ„àµ†àµ‡àµˆàµŠàµ‹àµŒàµàµ—àµºàµ»àµ¼àµ½àµ¾à¶‚à¶ƒà¶…à¶†à¶‡à¶ˆà¶‰à¶Šà¶‹à¶Œà¶à¶à¶à¶‘à¶’à¶“à¶”à¶•à¶–à¶šà¶›à¶œà¶à¶žà¶Ÿà¶ à¶¡à¶¢à¶£à¶¤à¶¥à¶¦à¶§à¶¨à¶©à¶ªà¶«à¶¬à¶­à¶®à¶¯à¶°à¶±à¶³à¶´à¶µà¶¶à¶·à¶¸à¶¹à¶ºà¶»à¶½à·€à·à·‚à·ƒà·„à·…à·†à·Šà·à·à·‘à·’à·“à·”à·–à·˜à·™à·šà·›à·œà·à·žà·Ÿà·´à¸à¸‚à¸ƒà¸„à¸…à¸†à¸‡à¸ˆà¸‰à¸Šà¸‹à¸Œà¸à¸Žà¸à¸à¸‘à¸’à¸“à¸”à¸•à¸–à¸—à¸˜à¸™à¸šà¸›à¸œà¸à¸žà¸Ÿà¸ à¸¡à¸¢à¸£à¸¤à¸¥à¸¦à¸§à¸¨à¸©à¸ªà¸«à¸¬à¸­à¸®à¸¯à¸°à¸±à¸²à¸³à¸´à¸µà¸¶à¸·à¸¸à¸¹à¸ºà¹€à¹à¹‚à¹ƒà¹„à¹…à¹†à¹‡à¹ˆà¹‰à¹Šà¹‹à¹Œà¹à¹Žà¹à¹›àºàº‚àº„àº‡àºˆàºŠàºàº”àº•àº–àº—àº™àºšàº›àºœàºàºžàºŸàº¡àº¢àº£àº¥àº§àºªàº«àº­àº®àº°àº±àº²àº³àº´àºµàº¶àº·àº¸àº¹àº»àº¼àº½à»€à»à»‚à»ƒà»„à»†à»ˆà»‰à»Šà»‹à»Œà»à»œà»à¼€à¼†à¼ˆà¼‹à¼Œà¼à¼Žà¼à¼‘à¼”à¼µà¼¼à¼½à½€à½à½‚à½„à½…à½†à½‡à½‰à½Šà½‹à½Œà½Žà½à½à½‘à½“à½”à½•à½–à½˜à½™à½šà½›à½à½žà½Ÿà½ à½¡à½¢à½£à½¤à½¥à½¦à½§à½¨à½ªà½¬à½±à½²à½´à½ºà½»à½¼à½½à½¾à½¿à¾€à¾‚à¾ƒà¾„à¾ˆà¾‹à¾à¾’à¾”à¾•à¾—à¾™à¾šà¾Ÿà¾¡à¾£à¾¤à¾¦à¾¨à¾©à¾«à¾­à¾°à¾±à¾²à¾³à¾µà¾¶à¾·à¿†á€€á€á€‚á€ƒá€„á€…á€†á€‡á€ˆá€‰á€Šá€‹á€Œá€á€Žá€á€á€‘á€’á€“á€”á€•á€–á€—á€˜á€™á€šá€›á€œá€á€žá€Ÿá€ á€¡á€¢á€£á€¤á€¥á€¦á€§á€¨á€©á€ªá€«á€¬á€­á€®á€¯á€°á€±á€²á€³á€´á€µá€¶á€·á€¸á€¹á€ºá€»á€¼á€½á€¾á€¿áŠá‹áŒááŽááá‘á’á“á”á•á—ášá›áœáážá á¡á¢á£á¤á¥á¦á§á¨áªá«á¬á­á°á±á²á³áµá¶á·á¸á¹áºá»á¼á½á¾á¿á‚€á‚á‚‚á‚ƒá‚„á‚…á‚†á‚‡á‚ˆá‚‰á‚Šá‚‹á‚Œá‚á‚Žá‚á‚šá‚œá‚á‚ á‚¢á‚£á‚¨á‚©á‚«á‚¬á‚®á‚°á‚µá‚¶á‚·á‚¿áƒáƒáƒ‘áƒ’áƒ“áƒ”áƒ•áƒ–áƒ—áƒ˜áƒ™áƒšáƒ›áƒœáƒáƒžáƒŸáƒ áƒ¡áƒ¢áƒ£áƒ¤áƒ¥áƒ¦áƒ§áƒ¨áƒ©áƒªáƒ«áƒ¬áƒ­áƒ®áƒ¯áƒ°áƒ±áƒ²áƒ³áƒ´áƒµáƒ¶áƒ·áƒ¹áƒºá„€á„…á„½á…®á†žáˆ€áˆáˆ‚áˆƒáˆ„áˆ…áˆ†áˆ‡áˆˆáˆ‰áˆŠáˆ‹áˆŒáˆáˆŽáˆáˆáˆ‘áˆ’áˆ“áˆ”áˆ•áˆ–áˆ—áˆ˜áˆ™áˆšáˆ›áˆœáˆáˆžáˆŸáˆ áˆ¡áˆ¢áˆ£áˆ¤áˆ¥áˆ¦áˆ§áˆ¨áˆ©áˆªáˆ«áˆ¬áˆ­áˆ®áˆ¯áˆ°áˆ±áˆ²áˆ³áˆ´áˆµáˆ¶áˆ·áˆ¸áˆ¹áˆºáˆ»áˆ¼áˆ½áˆ¾áˆ¿á‰€á‰á‰‚á‰ƒá‰„á‰…á‰†á‰‡á‰ˆá‰‹á‰Œá‰á‰‘á‰’á‰“á‰•á‰–á‰šá‰›á‰œá‰ á‰¡á‰¢á‰£á‰¤á‰¥á‰¦á‰§á‰¨á‰©á‰ªá‰«á‰¬á‰­á‰®á‰¯á‰°á‰±á‰²á‰³á‰´á‰µá‰¶á‰·á‰¸á‰¹á‰ºá‰»á‰¼á‰½á‰¾á‰¿áŠ€áŠáŠ‚áŠƒáŠ„áŠ…áŠ†áŠˆáŠŠáŠ‹áŠŒáŠáŠáŠ‘áŠ’áŠ“áŠ”áŠ•áŠ–áŠ—áŠ˜áŠ™áŠšáŠ›áŠœáŠáŠžáŠŸáŠ áŠ¡áŠ¢áŠ£áŠ¤áŠ¥áŠ¦áŠ§áŠ¨áŠ©áŠªáŠ«áŠ¬áŠ­áŠ®áŠ¯áŠ°áŠ²áŠ³áŠ´áŠµáŠ¸áŠ¹áŠºáŠ»áŠ¼áŠ½áŠ¾á‹‚á‹ƒá‹„á‹ˆá‹‰á‹Šá‹‹á‹Œá‹á‹Žá‹á‹á‹‘á‹’á‹“á‹”á‹•á‹–á‹˜á‹™á‹šá‹›á‹œá‹á‹žá‹Ÿá‹ á‹¡á‹¢á‹£á‹¤á‹¥á‹¦á‹¨á‹©á‹ªá‹«á‹¬á‹­á‹®á‹¯á‹°á‹±á‹²á‹³á‹´á‹µá‹¶á‹·á‹¸á‹¹á‹ºá‹»á‹¼á‹½á‹¾áŒ€áŒáŒ‚áŒƒáŒ„áŒ…áŒ†áŒ‡áŒˆáŒ‰áŒŠáŒ‹áŒŒáŒáŒŽáŒáŒáŒ’áŒ“áŒ”áŒ•áŒ˜áŒ™áŒšáŒ›áŒáŒŸáŒ áŒ¡áŒ¢áŒ£áŒ¤áŒ¥áŒ¦áŒ§áŒ¨áŒ©áŒªáŒ«áŒ¬áŒ­áŒ®áŒ¯áŒ°áŒ±áŒ²áŒ³áŒ´áŒµáŒ¶áŒ·áŒ¸áŒ¹áŒºáŒ»áŒ¼áŒ½áŒ¾á€áá‚áƒá„á…á†á‡áˆá‰áŠá‹áŒááŽááá‘á’á“á”á•á–á—á™ášááŸá á¡á¢á£á¤á¦áŽ€áŽáŽ‚áŽƒáŽ„áŽ…áŽ†áŽ‡áŽ‹áŽ áŽ¡áŽ¢áŽ£áŽ¤áŽ¥áŽ¦áŽ§áŽ¨áŽ©áŽªáŽ«áŽ¬áŽ­áŽ®áŽ¯áŽ°áŽ±áŽ²áŽ³áŽ´áŽµáŽ¶áŽ·áŽ¸áŽ¹áŽ»áŽ½áŽ¾áŽ¿á€áá‚áƒá„á…á†á‡á‰áŠá‹áŒááŽáá‘á’á“á”á•á–á—á˜á™ášá›áœáážáŸá á¡á¢á£á¤á¥á¦á§á¨á©áªá«á¬á®á¯á°á±á²á³á´á¸á¹áºá»á¼ááƒá„á…áŠáŽáá‘á’á˜áœááŸá¢á¤á¦á§á¨áªá¯á³á´á¶á¸áºá»á‘‚á‘†á‘‰á‘Šá‘‹á‘Œá‘Žá‘á‘‘á‘•á‘–á‘˜á‘›á‘œá‘¢á‘£á‘¤á‘¦á‘§á‘¨á‘ªá‘«á‘¬á‘­á‘®á‘¯á‘²á‘³á‘¶á‘·á‘¸á‘¹á‘¾á‘¿á’€á’á’‚á’ƒá’„á’†á’‰á’‹á’Œá’á’Žá’á’á’‘á’˜á’™á’šá’›á’œá’á’£á’¤á’¥á’¦á’§á’ªá’«á’¶á’·á’¸á’¹á’ºá’¼á“‚á“ƒá“„á“…á“‡á“ˆá“‹á“Œá“á“Žá“á“á“•á“—á“šá“¦á“§á“ªá“«á“¬á“®á“¯á“°á“±á“²á“´á“µá“ºá“¾á“¿á”…á”†á”á”‘á”“á”•á”–á”™á”šá”›á”œá”¡á”¢á”£á”¤á”¥á”©á”ªá”«á”­á”®á”·á”¼á”¿á•†á•ˆá•‰á•Šá•‹á•á•á•’á•“á•—á•˜á•™á•šá•žá• á•¢á•¦á•§á•¨á•©á•«á•¬á•®á•¯á•°á•±á•²á•³á•´á•µá•¶á•¼á•¾á•¿á–€á–…á–†á–‡á–á–‘á–’á–“á–”á–˜á–™á– á–«á–¯á–°á–±á–²á–³á–´á–¶á–¸á–ºá–»á–¼á–½á–¾á–¿á—á—‚á—…á—‹á—á—”á——á—šá—›á—œá—á—žá—Ÿá— á—¡á—¢á—¥á—¦á—¨á—©á—ªá—«á—¬á—­á—¯á—°á—±á—²á—³á—´á—µá—·á—¸á—¹á—¼á—½á—¾á—¿á˜‚á˜‰á˜á˜Žá˜á˜á˜‘á˜’á˜“á˜”á˜—á˜˜á˜™á˜œá˜á˜¢á˜®á˜³á˜´á˜¹á˜ºá˜»á˜¿á™€á™á™…á™ˆá™Šá™á™Žá™á™á™‘á™’á™“á™”á™–á™—á™˜á™™á™›á™œá™á™žá™Ÿá™¡á™¢á™£á™¤á™¥á™¦á™§á™ªá™«á™¬á™­ášŠáš áš¡áš¢áš£áš¤áš¦áš¨áš©ášªáš«áš¬áš®áš±áš²áš³áš´ášµáš·áš¹ášºáš»áš¼áš¾áš¿á›á›‚á›ƒá›„á›†á›‡á›ˆá›‰á›Šá›‹á›á›á›á›‘á›’á›–á›—á›˜á›šá›á›žá›Ÿá›£á›«áœ€áœáœ‚áœƒáœ„áœ…áœ†áœ‡áœˆáœ‰áœŠáœ‹áœŒáœŽáœáœáœ‘áœ’áœ“áœ”áž€ážáž‚ážƒáž„áž…áž†áž‡ážˆáž‰ážŠáž‹ážŒážážŽážážáž‘áž’áž“áž”áž•áž–áž—áž˜áž™ážšáž›ážœážážžážŸáž áž¡áž¢áž£áž¥áž¦áž§áž¨áž©ážªáž«áž¬áž­áž®áž¯áž°áž±áž²áž³áž´áž¶áž·áž¸áž¹ážºáž»áž¼áž½áž¾áž¿áŸ€áŸáŸ‚áŸƒáŸ„áŸ…áŸ†áŸ‡áŸˆáŸ‰áŸŠáŸ‹áŸŒáŸáŸŽáŸáŸáŸ‘áŸ’áŸ“áŸ”áŸ–áŸ—áŸšá Œá  á ¡á ¢á £á ¤á ¥á ¦á §á ¨á ©á ªá «á ¬á ­á ®á ¯á °á ±á ²á ³á ´á µá ¶á ·á ¸á ¾á¡ƒá¡„á¡…á¡‡á¡‰á¡‘á¡•á¡ á¡¤á¡¨á¡©á¡³á¡µá¢†á¢šá¢¤á¢¨á£ƒá£„á¤€á¤á¤‚á¤ƒá¤…á¤†á¤‡á¤ˆá¤‹á¤Œá¤á¤Žá¤á¤á¤‘á¤’á¤“á¤”á¤•á¤–á¤—á¤˜á¤™á¤›á¤œá¤ á¤¡á¤¢á¤£á¤¥á¤§á¤©á¤ªá¤°á¤±á¤´á¤¶á¤·á¤»á¥„á¥á¥‘á¥“á¥”á¥•á¥–á¥—á¥™á¥›á¥œá¥á¥žá¥ á¥¡á¥¢á¥£á¥¦á¥¨á¥©á¥ªá¥«á¥­á¥°á¥±á¥²á¥´á¦á¦’á¨€á¨‚á¨„á¨…á¨†á¨ˆá¨‰á¨Šá¨á¨á¨‘á¨’á¨“á¨”á¨•á¨–á¨—á¨™á¨šá©Žá¬¡á¬¥á¬¦á¬§á¬¬á¬¯á¬¾á­„á®á®‚á®ƒá®„á®…á®Œá®’á®“á®”á®•á®–á®™á®›á®œá®á®žá®Ÿá®¤á®¥á®¨á®ªá®¼á®½á¯‡á¯¤á¯©á¯±á°„á°­á°¯á°±á±šá±›á±œá±á±žá±Ÿá± á±¡á±¢á±£á±¤á±¥á±¦á±§á±¨á±©á±«á±¬á±­á±®á±¯á±°á±±á±²á±³á±´á±µá±¶á±·á±¸á±¹á±½á´€á´„á´…á´†á´‡á´‰á´Šá´‹á´á´á´‘á´“á´–á´—á´˜á´™á´šá´›á´œá´Ÿá´ á´¢á´¥á´§á´¨á´«á´¬á´®á´°á´±á´²á´³á´´á´µá´¶á´·á´¸á´ºá´»á´¼á´½á´¿áµ€áµáµ…áµ‡áµáµáµ”áµœáµžáµ¡áµ©áµªáµ¯áµ´áµµáµ¸á¶’á¶“á¶”á¶•á¶œá¶ á¶¢á¶¤á¶¨á¶¯á¶°á¶¸á¶»á·ˆá·‰á·Šá·‹á·á·á·žá·Ÿá·§á·½á·¾á·¿á¸€á¸á¸‚á¸ƒá¸„á¸…á¸†á¸‡á¸ˆá¸‰á¸Šá¸‹á¸Œá¸á¸Žá¸á¸á¸‘á¸’á¸“á¸”á¸•á¸–á¸—á¸˜á¸™á¸šá¸›á¸œá¸á¸žá¸Ÿá¸ á¸¡á¸¢á¸£á¸¤á¸¥á¸¦á¸§á¸¨á¸©á¸ªá¸«á¸¬á¸­á¸®á¸¯á¸°á¸±á¸²á¸³á¸´á¸µá¸¶á¸·á¸¸á¸¹á¸ºá¸»á¸¼á¸½á¸¾á¸¿á¹€á¹á¹‚á¹ƒá¹„á¹…á¹†á¹‡á¹ˆá¹‰á¹Šá¹‹á¹Œá¹á¹Žá¹á¹á¹‘á¹’á¹“á¹”á¹•á¹–á¹—á¹˜á¹™á¹šá¹›á¹œá¹á¹žá¹Ÿá¹ á¹¡á¹¢á¹£á¹¤á¹¥á¹¦á¹§á¹¨á¹©á¹ªá¹«á¹¬á¹­á¹®á¹¯á¹°á¹±á¹²á¹³á¹´á¹µá¹¶á¹·á¹¸á¹¹á¹ºá¹»á¹¼á¹½á¹¾á¹¿áº€áºáº‚áºƒáº„áº…áº†áº‡áºˆáº‰áºŠáº‹áºŒáºáºŽáºáºáº‘áº’áº“áº”áº•áº–áº—áº˜áº™áºšáº›áºœáºáºžáºŸáº áº¡áº¢áº£áº¤áº¥áº¦áº§áº¨áº©áºªáº«áº¬áº­áº®áº¯áº°áº±áº²áº³áº´áºµáº¶áº·áº¸áº¹áººáº»áº¼áº½áº¾áº¿á»€á»á»‚á»ƒá»„á»…á»†á»‡á»ˆá»‰á»Šá»‹á»Œá»á»Žá»á»á»‘á»’á»“á»”á»•á»–á»—á»˜á»™á»šá»›á»œá»á»žá»Ÿá» á»¡á»¢á»£á»¤á»¥á»¦á»§á»¨á»©á»ªá»«á»¬á»­á»®á»¯á»°á»±á»²á»³á»´á»µá»¶á»·á»¸á»¹á»ºá»»á»¼á»½á»¾á»¿á¼€á¼á¼‚á¼ƒá¼„á¼…á¼†á¼‡á¼ˆá¼‰á¼Œá¼á¼á¼á¼‘á¼’á¼“á¼”á¼•á¼˜á¼™á¼›á¼œá¼á¼ á¼£á¼¤á¼¥á¼§á¼¨á¼©á¼ªá¼¬á¼­á¼¯á¼°á¼±á¼²á¼³á¼´á¼µá¼¶á¼¸á¼¹á¼¼á½€á½á½‚á½ƒá½„á½ˆá½‰á½‹á½á½“á½”á½•á½–á½—á½™á½›á½á½ á½£á½¦á½§á½®á½¯á½°á½´á½µá½¶á½¸á½ºá½¼á¾€á¾‚á¾ƒá¾„á¾†á¾‡á¾Šá¾‹á¾Œá¾á¾á¾‘á¾•á¾–á¾—á¾˜á¾á¾Ÿá¾¤á¾¥á¾¬á¾°á¾±á¾´á¾¶á¾·á¾¸á¾»á¿‚á¿†á¿Šá¿á¿‘á¿’á¿–á¿¡á¿¢á¿¥á¿¦á¿¨á¿¬á¿³á¿´á¿¶á¿·á¿ºâ€“â€”â€—â€˜â€™â€šâ€œâ€â€žâ€ â€¡â€¢â€¦â€§â€°â€¹â€ºâ€»â€¼â‚â±â‚Žâ‚“â‚¥â‚¬â‚¯âƒ’âƒ”âƒ•âƒ–âƒ—âƒ˜âƒšâƒœâƒâƒžâƒŸâƒ âƒ¡âƒ¢âƒ£âƒ¤âƒ¦âƒ§âƒ©âƒ«âƒ¬âƒ­âƒ®âƒ¯â„‚â„…â„‡â„‘â„’â„“â„›â„ â„¢â„¦â„¬â„¯â„±â„´â……â…¡â…¢â…´â†¯âˆ‚âˆ‘âˆžâˆ©âˆ«â‰ˆâ‰ â‰¡â‰¤â‰¥â‰¦â‰§âŠ•âŠ°âŠ±â‹â‹†â‹‹â‹ŒâŒ£âŽ›âŽâŽžâŽ âŽ²âŽµâŽ·âŽââ â‘©â‘«â‘°â’¶â’·â’¸â’ºâ’»â’¿â“€â“‚â“„â“Šâ“‹â“Œâ“â“â““â“–â“šâ“›â“â“Ÿâ“¦â“¨â“«â”€â”â”‚â”Œâ”â””â”˜â”œâ”¤â”¼â•â•‘â•”â•—â•˜â•šâ•›â•â•£â•¤â•¦â•§â•©â•ªâ•¬â•­â•®â•¯â•°â–â–‚â–ƒâ–…â–†â–‡â–ˆâ–‘â–’â–“â–·â–ºâ–¼â—â—„â—‡â—Šâ—’â—”â—˜â—¡â—¢â—¤â—ªâ˜€â˜…â˜†â˜Šâ˜œâ˜¡â˜¥â˜­â˜®â˜ºâ˜¼â˜¾â™ˆâ™”â™›â™¡â™£â™ªâœ‚âœ…âœ‡âœŒâœŽâœ“âœ”âœ—âœ˜âœâœžâœŸâœ£âœ¤âœ¥âœ¦âœ©âœªâœ«âœ¬âœ­âœ®âœ¯âœ°âœ±âœ²âœ¹âœ¿â€ââ‚âƒâ„â‰âŠâ£â¤â¥â¦â§â¿âžœâž¸â €â£§â±¢â±¤â±¥â±¦â±­â±®â±¯â±¾â±¿â´‡â´ˆâ´Œâ´Ÿâ´°â´±â´²â´³â´´â´µâ´¶â´·â´¸â´¹â´ºâ´»â´¼â´½â´¾â´¿âµ€âµâµ‚âµƒâµ„âµ…âµ†âµ‡âµˆâµ‰âµŠâµ‹âµâµŽâµâµâµ’âµ“âµ”âµ•âµ–âµ—âµ˜âµ™âµšâµ›âµœâµâµžâµŸâµ âµ¡âµ¢âµ£âµ¤âµ¥âµ§âµ¯â·¡â·¥â·¦â·ªâ·­â·®â·¯â·´âºŒã€ã€‚ã€…ã€†ã€ˆã€‰ã€Šã€‹ã€Œã€ã€Žã€ã€ã€‘ã€˜ã€™ã€œã€¤ãã‚ãƒã„ã…ã†ã‡ãˆã‰ãŠã‹ãŒããŽããã‘ã’ã“ã”ã•ã–ã—ã˜ã™ãšã›ãœããžãŸã ã¡ã¢ã£ã¤ã¥ã¦ã§ã¨ã©ãªã«ã¬ã­ã®ã¯ã°ã±ã²ã³ã´ãµã¶ã·ã¸ã¹ãºã»ã¼ã½ã¾ã¿ã‚€ã‚ã‚‚ã‚ƒã‚„ã‚…ã‚†ã‚‡ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚Žã‚ã‚ã‚‘ã‚’ã‚“ã‚”ã‚–ã‚ã‚žã‚¡ã‚¢ã‚£ã‚¤ã‚¥ã‚¦ã‚§ã‚¨ã‚©ã‚ªã‚«ã‚¬ã‚­ã‚®ã‚¯ã‚°ã‚±ã‚²ã‚³ã‚´ã‚µã‚¶ã‚·ã‚¸ã‚¹ã‚ºã‚»ã‚¼ã‚½ã‚¾ã‚¿ãƒ€ãƒãƒ‚ãƒƒãƒ„ãƒ…ãƒ†ãƒ‡ãƒˆãƒ‰ãƒŠãƒ‹ãƒŒãƒãƒŽãƒãƒãƒ‘ãƒ’ãƒ“ãƒ”ãƒ•ãƒ–ãƒ—ãƒ˜ãƒ™ãƒšãƒ›ãƒœãƒãƒžãƒŸãƒ ãƒ¡ãƒ¢ãƒ£ãƒ¤ãƒ¥ãƒ¦ãƒ§ãƒ¨ãƒ©ãƒªãƒ«ãƒ¬ãƒ­ãƒ®ãƒ¯ãƒ°ãƒ±ãƒ²ãƒ³ãƒ´ãƒµãƒ¶ãƒ·ãƒ»ãƒ¼ãƒ½ãƒ¾ã„Šã„Žã„“ã„ã„žã„¨ã„±ã„´ã„¹ã…ã…‡ã…ˆã…‰ã…ã…Žã…ã…“ã…”ã…–ã…—ã…œã…¡ã…£ã…¤ã†ãŠ–ãŠ£ã‹–ã‹›ã‹¡ã”¶ã•¥ã›„ã£„ã©§ã©®ã®ˆãº©ä‰€ä‘‚ä‘“ä•ƒä­“ä¸€ä¸ä¸‚ä¸ƒä¸…ä¸‡ä¸ˆä¸‰ä¸Šä¸‹ä¸ä¸Žä¸‘ä¸’ä¸“ä¸”ä¸•ä¸–ä¸˜ä¸™ä¸šä¸›ä¸œä¸ä¸žä¸Ÿä¸¡ä¸¤ä¸¥ä¸¦ä¸¨ä¸ªä¸«ä¸­ä¸°ä¸²ä¸´ä¸¶ä¸·ä¸¸ä¸¹ä¸ºä¸»ä¸¼ä¸½ä¸¿ä¹€ä¹‚ä¹ƒä¹„ä¹…ä¹‡ä¹ˆä¹‰ä¹‹ä¹Œä¹ä¹Žä¹ä¹’ä¹“ä¹”ä¹–ä¹—ä¹˜ä¹™ä¹›ä¹œä¹ä¹Ÿä¹ ä¹¡ä¹¦ä¹°ä¹±ä¹³ä¹¾äº€äº‚äº†äºˆäº‰äº‹äºŒäºŽäºäº‘äº’äº“äº”äº•äº–äº—äº˜äº™äºšäº›äºœäºžäºŸäº äº¡äº¢äº¤äº¥äº¦äº§äº¨äº©äº«äº¬äº­äº®äº°äº²äºµäº·äººäº»äº¼äº½äº¾äº¿ä»€ä»ä»…ä»‡ä»ˆä»Šä»‹ä»Œä»ä»Žä»ä»“ä»”ä»•ä»˜ä»™ä»šä»ä»Ÿä»£ä»¤ä»¥ä»¨ä»ªä»¬ä»®ä»°ä»²ä»¶ä»·ä»ºä»»ä»½ä¼ä¼ˆä¼‰ä¼Šä¼ä¼ä¼‘ä¼—ä¼˜ä¼™ä¼šä¼ä¼žä¼Ÿä¼ ä¼¤ä¼¦ä¼¯ä¼±ä¼´ä¼¶ä¼¸ä¼ºä¼¼ä¼½ä½†ä½ä½Žä½ä½ä½‘ä½“ä½•ä½–ä½˜ä½™ä½›ä½œä½ ä½£ä½©ä½¬ä½°ä½³ä½¶ä½¿ä¾ƒä¾†ä¾‹ä¾ä¾ä¾‘ä¾–ä¾›ä¾ä¾žä¾ ä¾¡ä¾£ä¾¥ä¾§ä¾¨ä¾¯ä¾¶ä¾¿ä¿‚ä¿ƒä¿„ä¿Šä¿ä¿—ä¿™ä¿šä¿ä¿žä¿ ä¿¡ä¿£ä¿ªä¿¬ä¿­ä¿®ä¿¯ä¿µä¿ºä¿½å€‰å€‹å€å€’å€–å€™å€šå€Ÿå€¡å€¤å€¦å€©å€ªå€«å€­å€¸å€»å€¼å€¾å‡åˆå‰åŒååšåœå¥å©å²å³å´åµå¶å·å½å‚€å‚…å‚å‚‘å‚˜å‚™å‚¢å‚¬å‚­å‚²å‚³å‚µå‚·å‚»å‚¾åƒåƒåƒ‘åƒ•åƒ–åƒšåƒ¡åƒ®åƒ°åƒ±åƒµåƒ¹åƒ¾å„€å„„å„‰å„å„’å„Ÿå„¡å„ªå„·å„¸å„¿å…å…ƒå…„å……å…†å…ˆå…‰å…‹å…å…Žå…å…’å…”å…šå…œå…¥å…§å…¨å…©å…«å…¬å…­å…®å…°å…±å…²å…³å…´å…µå…¶å…·å…¸å…¹å…»å…¼å…½å†€å†„å†…å††å†‡å†ˆå†‰å†Šå†‹å†Œå†å†’å†–å†™å†›å†œå† å†¢å†¥å†§å†¨å†¬å†­å†¯å†°å†²å†³å†´å†¶å†·å†»å†¼å†½å‡€å‡„å‡†å‡‰å‡Šå‡Œå‡å‡å‡˜å‡›å‡œå‡å‡žå‡ å‡¡å‡¤å‡¦å‡ªå‡¯å‡°å‡±å‡³å‡¸å‡ºå‡»å‡½åˆ€åˆåˆƒåˆ„åˆ†åˆ‡åˆˆåˆŠåˆ‘åˆ’åˆ—åˆ˜åˆ™åˆšåˆ›åˆåˆ åˆ¤åˆ¥åˆ©åˆªåˆ«åˆ®åˆ°åˆ¶åˆ¸åˆ¹åˆºåˆ»å‰€å‰‡å‰Šå‰Œå‰å‰Žå‰‘å‰”å‰›å‰¡å‰£å‰¤å‰¯å‰±å‰²å‰´å‰µåŠ‡åŠ‰åŠåŠåŠ‘åŠ›åŠžåŠŸåŠ åŠ¡åŠ£åŠ¨åŠ©åŠªåŠ«åŠ¬åŠ­åŠ±åŠ²åŠ³åŠ¹åŠ¼åŠ¿å‹å‹ƒå‹…å‹‡å‹‰å‹‹å‹å‹’å‹•å‹˜å‹™å‹›å‹å‹žå‹Ÿå‹¢å‹¤å‹²å‹³å‹µå‹»å‹¿åŒ€åŒ‚åŒ…åŒ’åŒ•åŒ–åŒ—åŒ™åŒšåŒ åŒ¡åŒªåŒ¯åŒ±åŒ¹åŒºåŒ»åŒ¿å€ååƒå„å‡åˆå‰åŠåŒåŽåå‘å“å”å•å–å—å˜åšåœååžå å¡å¢å£å¦å§å«å¯å°å±å³åµå·å½å¿åŽ‚åŽ„åŽ†åŽŒåŽ˜åŽšåŽŸåŽ åŽ¤åŽ¦åŽ¨åŽ­åŽ²åŽ³åŽ¶åŽ·åŽ»åå‚åƒå„åˆå‰åŠå‹åŒååŽå‘å”å–å—å˜å™å›å¡å£å¤å¥å¦åªå«å¬å­å®å¯å°å²å³å¶å·å¸å»å¼åƒå„å‡åˆå‰å‹åŒååŽååå‘å•å—å›åŸå¡å¥å¦å§å«å¬å®å¯å±å³å´åµå¸å¹å»å¼å¾å‘€å‘‚å‘†å‘‡å‘ˆå‘‰å‘Šå‘å‘‘å‘’å‘“å‘–å‘—å‘˜å‘¢å‘¦å‘¨å‘ªå‘±å‘³å‘µå‘¼å‘½å’€å’å’‚å’„å’‹å’Œå’å’å’”å’•å’–å’˜å’šå’£å’©å’ªå’¯å’²å’³å’½å’¿å“€å“å“†å“‡å“ˆå“‰å“‹å“å“Žå“’å““å“”å“Ÿå“¡å“¥å“©å“ªå“­å“®å“²å”„å”†å”‡å”‰å”å”å””å”–å”œå”§å”®å”¯å”±å”²å”¹å•„å•†å•Šå•å•“å•Ÿå•¡å•¤å•¦å•µå•¸å•¼å•¾å–€å–ƒå–„å–†å–‡å–‰å–Šå–”å–˜å–œå–å–ªå–¬å–®å–°å–±å–µå–¶å–·å–»å—Žå—å—–å—šå—œå—£å—¦å—ªå—«å—¯å—±å˜‰å˜Žå˜˜å˜šå˜›å˜œå˜Ÿå˜ å˜©å˜­å˜¯å˜²å˜»å˜¿å™Œå™—å™›å™œå™ å™¢å™¨å™´å™¶å™¹åš‡åšåš•åš§åš®åš´åš¼å›‰å›Šå›å›—å›šå››å›å›žå›Ÿå› å›¡å›¢å›£å›§å›­å›¯å›°å›³å›¹å›ºå›½å›¾åœƒåœ†åœˆåœ‹åœåœ’åœ“åœ–åœ˜åœŸåœ£åœ§åœ¨åœ­åœ°åœ³åœºå€å‚å‡åŠåŽååå‘å—å™åšå›åå¡å¤å¦åªå­å·åž‚åž‹åžšåž åž¢åž£åž«åž²åŸƒåŸŽåŸ”åŸœåŸŸåŸ´åŸ·åŸ¹åŸºåŸ¼å €å ‚å ƒå …å ‡å Šå •å ¡å ¤å ªå ¯å °å ±å ´å µå ºå¡å¡Šå¡‹å¡‘å¡”å¡—å¡˜å¡™å¡šå¡žå¡©å¡«å¡±å¡²å¡µå¢ƒå¢…å¢Šå¢“å¢—å¢™å¢žå¢¨å¢®å¢³å¢¾å£å£‡å£Šå£•å£˜å£žå£¤å£«å£®å£¯å£°å£±å£²å£³å£¶å£·å£¹å£ºå£½å¤„å¤‡å¤‰å¤Šå¤Œå¤å¤å¤•å¤–å¤˜å¤šå¤›å¤œå¤Ÿå¤ å¤¢å¤§å¤©å¤ªå¤«å¤¬å¤®å¤¯å¤±å¤´å¤·å¤ºå¥€å¥„å¥‡å¥ˆå¥‰å¥‹å¥Žå¥å¥å¥‘å¥”å¥•å¥–å¥—å¥ å¥¢å¥¥å¥§å¥¨å¥ªå¥®å¥³å¥´å¥µå¥¶å¥¸å¥½å¦‚å¦ƒå¦„å¦†å¦‡å¦å¦å¦’å¦–å¦—å¦˜å¦™å¦å¦¤å¦¥å¦¨å¦©å¦®å¦³å¦¹å¦ºå¦»å§†å§‰å§‹å§å§å§‘å§“å§”å§—å§šå§œå§å§¨å§«å§¬å§²å§³å§µå§¶å§·å§¸å§¿å¨å¨ƒå¨…å¨‡å¨‰å¨’å¨˜å¨›å¨œå¨Ÿå¨£å¨¥å¨«å¨³å¨´å©€å©†å©‰å©•å©—å©šå©žå©¦å©§å©­å©³å©´å©µå©·åªåª‚åª’åª–åªšåª›åªœåª½å«å«‚å«Œå«’å«™å«šå«£å«¦å«©å«²å«»å¬…å¬‰å¬Šå¬‹å¬Œå¬¡å¬£å¬¤å¬ªå¬°å¬·å¬¸å¬¿å­ƒå­‹å­å­‘å­”å­–å­—å­˜å­™å­›å­œå­å­Ÿå­£å­¤å­¦å­©å­«å­¬å­±å­¸å­½å®å®ƒå®…å®‡å®ˆå®‰å®‹å®Œå®å®å®“å®—å®˜å®™å®šå®›å®œå®å®žå®Ÿå®¢å®£å®¤å®¥å®§å®ªå®«å®¬å®®å®°å®²å®µå®¶å®¸å®¹å®½å®¾å®¿å¯å¯‚å¯„å¯…å¯†å¯‡å¯Œå¯å¯’å¯“å¯˜å¯›å¯å¯žå¯Ÿå¯¡å¯¦å¯§å¯©å¯«å¯¬å¯®å¯¯å¯°å¯³å¯µå¯¶å¯¸å¯¹å¯ºå¯»å¯¼å¯¾å¯¿å°å°‚å°„å°†å°‡å°ˆå°‰å°Šå°‹å°å°Žå°å°å°‘å°’å°”å°•å°–å°˜å°šå°›å°¤å°§å°­å°±å°¸å°¹å°ºå°»å°¼å°½å°¾å±€å±å±‚å±…å±ˆå±Šå±‹å±å±å±å±‘å±•å±å±žå± å±¤å±¬å±¯å±±å±ºå²å²„å²å²‘å²šå²›å²¡å²©å²«å²¬å²­å²±å²³å²¸å³‡å³ å³¡å³¤å³¥å³¨å³ªå³­å³®å³¯å³°å³¶å³»å´‡å´Šå´‹å´Žå´‘å´”å´–å´šå´¤å´©å´´å´µåµ‹åµåµ˜åµ›åµœåµ©åµ¬åµ¯åµ»å¶‹å¶Œå¶ºå¶¼å¶½å·å·’å·–å·›å·å·žå· å·¡å·¢å·£å·¥å·¦å·§å·¨å·«å·®å·±å·²å·³å·´å··å·»å·¾å¸å¸‚å¸ƒå¸…å¸†å¸ˆå¸Œå¸å¸•å¸–å¸˜å¸å¸¥å¸¦å¸«å¸­å¸®å¸¯å¸°å¸³å¸¶å¸·å¸¸å¸¼å¸½å¹…å¹Œå¹•å¹Ÿå¹¡å¹£å¹«å¹²å¹³å¹´å¹¶å¹¸å¹¹å¹»å¹¼å¹½å¹¾å¹¿åºƒåº„åº†åºŠåºåº“åº”åº•åº—åº™åºšåºœåºžåº åº¦åº§åº«åº­åºµåº·åº¸å»å»‚å»„å»‰å»Šå»–å»šå»Ÿå» å»¢å»£å»³å»´å»¶å»·å»¸å»ºå»»å»¾å»¿å¼€å¼å¼‚å¼„å¼‹å¼å¼Žå¼å¼å¼‘å¼’å¼“å¼”å¼•å¼—å¼˜å¼›å¼Ÿå¼ å¼¥å¼¦å¼©å¼¯å¼±å¼µå¼·å¼¸å¼¹å¼ºå¼¼å¼¾å½ˆå½Œå½Žå½‘å½’å½“å½•å½—å½¡å½¢å½£å½¤å½¥å½¦å½§å½©å½ªå½«å½¬å½­å½°å½±å½¹å½»å½¼å½¾å¾€å¾å¾„å¾…å¾ˆå¾‹å¾Œå¾å¾‘å¾’å¾“å¾—å¾™å¾žå¾¡å¾¦å¾©å¾ªå¾®å¾¯å¾³å¾´å¾µå¾·å¾¹å¾½å¿ƒå¿„å¿…å¿†å¿Œå¿å¿—å¿˜å¿™å¿žå¿ å¿§å¿«å¿±å¿²å¿µå¿¸å¿»å¿½æ€€æ€æ€Žæ€’æ€–æ€œæ€æ€ æ€¡æ€¥æ€§æ€¨æ€©æ€ªæ€·æ€ºæ€»æ€¿æ†æ‹ææ’æ•æ›æ¢æ¥æ¨æ©æ¬æ­æ¯æ°æµæºæ‚…æ‚‰æ‚Œæ‚”æ‚–æ‚Ÿæ‚ æ‚¤æ‚¦æ‚©æ‚ªæ‚¬æ‚²æ‚³æ‚´æ‚¶æƒ…æƒ‡æƒŠæƒ‘æƒ˜æƒœæƒŸæƒ æƒ¡æƒ£æƒªæƒ¯æƒ°æƒ³æƒ¹æƒºæ„æ„ˆæ„‰æ„æ„šæ„›æ„Ÿæ„«æ„·æ„¼æ„¿æ…ˆæ…Šæ…‹æ…Œæ…Žæ…•æ…˜æ…œæ…¢æ…£æ…§æ…¨æ…®æ…°æ…¶æ…·æ†‚æ†Žæ†æ†”æ†¤æ†§æ†«æ†¬æ†²æ†¶æ†¾æ‡‚æ‡‰æ‡‹æ‡Œæ‡æ‡’æ‡“æ‡¦æ‡¶æ‡·æ‡¸æ‡¿æˆ€æˆ…æˆ‡æˆˆæˆŽæˆæˆæˆ‘æˆ’æˆ–æˆ˜æˆ›æˆ¦æˆ®æˆ¯æˆ°æˆ²æˆ´æˆ¶æˆ·æˆ¸æˆ¿æ‰€æ‰æ‰†æ‰‡æ‰‹æ‰æ‰Žæ‰‘æ‰’æ‰“æ‰˜æ‰›æ‰£æ‰§æ‰©æ‰¬æ‰­æ‰¯æ‰³æ‰¶æ‰¹æ‰¾æ‰¿æŠ€æŠ„æŠŠæŠ’æŠ“æŠ•æŠ–æŠ—æŠ˜æŠšæŠžæŠ¢æŠ¤æŠ¥æŠ±æŠµæŠ¹æŠ¼æŠ½æ‹…æ‹†æ‹ˆæ‹‰æ‹æ‹Žæ‹’æ‹“æ‹”æ‹›æ‹œæ‹æ‹¡æ‹¥æ‹¨æ‹©æ‹¬æ‹¯æ‹±æ‹³æ‹¼æ‹½æ‹¾æ‹¿æŒæŒ‡æŒ‰æŒ‘æŒ™æŒ¨æŒªæŒ¯æŒºæŒ½æŒ¾æŒææ¢æ¨æ®æ²æ·æºæŽƒæŽ„æŽˆæŽŒæŽ’æŽ˜æŽ›æŽžæŽ æŽ¡æŽ¢æŽ£æŽ¥æŽ§æŽ¨æŽªæŽ¬ææ‰æææ–æšæ›æ¡æªæ®æ´æ¼ææœæ¬æ­æµæ¶æºæ‘‚æ‘„æ‘†æ‘˜æ‘Ÿæ‘©æ‘³æ‘¸æ‘ºæ’ƒæ’ˆæ’’æ’“æ’žæ’¤æ’«æ’­æ’®æ’²æ’³æ’¼æ“æ“‚æ“‡æ“Šæ“‹æ“æ“”æ“¦æ“´æ“ºæ”æ”ªæ”¯æ”µæ”¶æ”¸æ”¹æ”»æ”¾æ”¿æ•…æ•ˆæ•‰æ•Œæ•æ•‘æ•—æ•™æ•¢æ•£æ•¦æ•¬æ•°æ•´æ•µæ•·æ•¸æ–‡æ–ˆæ–‰æ–‹æ–Œæ–Žæ–æ–‘æ–—æ–™æ–œæ–¡æ–¤æ–§æ–©æ–¬æ–­æ–¯æ–°æ–³æ–·æ–¹æ–¼æ–½æ–¿æ—…æ—†æ—‹æ—Œæ—Žæ—æ—–æ——æ— æ—¢æ—¤æ—¥æ—¦æ—§æ—¨æ—©æ—¬æ—­æ—±æ—¶æ—·æ—ºæ—»æ—¼æ˜€æ˜‚æ˜†æ˜‡æ˜‰æ˜Šæ˜Œæ˜Žæ˜æ˜“æ˜•æ˜œæ˜Ÿæ˜ æ˜¥æ˜§æ˜¨æ˜­æ˜¯æ˜°æ˜±æ˜´æ˜¼æ˜¾æ™‚æ™ƒæ™‰æ™‹æ™æ™“æ™–æ™—æ™šæ™žæ™Ÿæ™¤æ™§æ™¨æ™®æ™¯æ™°æ™´æ™¶æ™ºæšæš„æš‡æšˆæš‰æšæš‘æš–æš—æš˜æš¢æš§æš«æš®æš´æš¹æ›„æ›‡æ›ˆæ›‰æ›“æ›–æ›™æ›œæ›¦æ›°æ›²æ›³æ›´æ›µæ›¸æ›¹æ›ºæ›»æ›¼æ›½æ›¾æ›¿æœ€æœƒæœˆæœ‰æœ‹æœæœ”æœ•æœ—æœ›æœæœŸæœ£æœ§æœ¨æœ©æœªæœ«æœ¬æœ­æœ¯æœ±æœ´æœµæœºæœ½æ‚æƒæ†æ‰æŽæææ‘æ–æœæžæŸæ¡æ¢æ£æ¥æ¨æ­æ¯æ°æ±æµæºæ¾æ¿æžæž‹æžæžæž’æž—æž˜æžšæžœæžæž æž¡æž¦æž«æž­æž¯æž°æž¶æŸæŸƒæŸ„æŸŠæŸæŸæŸ‘æŸ’æŸ“æŸ”æŸ˜æŸšæŸœæŸžæŸ æŸ¥æŸ¯æŸ±æŸ³æŸ´æŸµæŸ»æŸ¾æŸ¿æ æ ‚æ ƒæ „æ ‡æ ˆæ ‹æ Žæ ‘æ “æ –æ —æ æ žæ ¡æ ¢æ ¤æ ©æ µæ ·æ ¸æ ¹æ ¼æ ½æ¡€æ¡æ¡‚æ¡ƒæ¡ˆæ¡‰æ¡Šæ¡Œæ¡æ¡‘æ¡’æ¡“æ¡”æ¡œæ¡æ¡Ÿæ¡¢æ¡£æ¡¥æ¡¦æ¡§æ¡©æ¡¶æ¢æ¢…æ¢“æ¢˜æ¢›æ¢æ¢Ÿæ¢ æ¢¢æ¢¦æ¢§æ¢¨æ¢­æ¢¯æ¢°æ¢µæ¢¶æ¢¿æ£„æ£‰æ£‹æ£’æ£•æ£—æ£˜æ£šæ£Ÿæ£ æ£§æ£®æ£±æ£»æ£¿æ¤€æ¤…æ¤‹æ¤æ¤Žæ¤’æ¤™æ¤›æ¤œæ¤°æ¤½æ¤¿æ¥Šæ¥’æ¥“æ¥šæ¥œæ¥ æ¥¡æ¥¢æ¥§æ¥¨æ¥­æ¥¯æ¥³æ¥µæ¥·æ¥¸æ¥¹æ¥¼æ¥½æ¦‚æ¦†æ¦ˆæ¦Šæ¦Žæ¦‘æ¦•æ¦›æ¦œæ¦Ÿæ¦®æ¦´æ¦ºæ§€æ§æ§‡æ§‹æ§Œæ§˜æ§™æ§ºæ§»æ§½æ¨‚æ¨Šæ¨‹æ¨‘æ¨“æ¨—æ¨™æ¨¡æ¨£æ¨©æ¨ªæ¨«æ¨±æ¨¹æ¨ºæ¨½æ©„æ©‹æ©˜æ©™æ©žæ©Ÿæ©¡æ©«æª€æªŽæª”æªœæª©æªªæª¬æª³æª¸æ«›æ«Ÿæ«¥æ«²æ«»æ¬„æ¬…æ¬Šæ¬–æ¬ æ¬¡æ¬¢æ¬£æ¬§æ¬²æ¬¸æ¬ºæ¬½æ¬¾æ­†æ­Œæ­æ­¡æ­¢æ­£æ­¤æ­¥æ­¦æ­©æ­ªæ­¯æ­³æ­´æ­·æ­¸æ­»æ®‡æ®‰æ®Šæ®‹æ®˜æ®¡æ®¤æ®­æ®¯æ®´æ®µæ®·æ®ºæ®»æ®¼æ®¿æ¯…æ¯‰æ¯æ¯Žæ¯æ¯“æ¯”æ¯˜æ¯›æ¯«æ¯¬æ°æ°‘æ°“æ°”æ°•æ°—æ°›æ°¢æ°£æ°´æ°µæ°·æ°¸æ°¹æ±€æ±æ±‚æ±‰æ±Žæ±æ±“æ±•æ±—æ±šæ±æ±Ÿæ± æ±¡æ±¤æ±ªæ±°æ±²æ±¶æ±ºæ±½æ±¾æ²æ²‚æ²ƒæ²„æ²…æ²ˆæ²‰æ²æ²’æ²“æ²–æ²™æ²šæ²›æ²¡æ²¢æ²§æ²ªæ²«æ²¬æ²­æ²³æ²µæ²¸æ²¹æ²ºæ²»æ²¼æ³æ³‰æ³Šæ³‹æ³“æ³•æ³—æ³žæ³ æ³¡æ³¢æ³£æ³¥æ³¨æ³ªæ³«æ³¯æ³°æ³³æ³µæ³¼æ³½æ´æ´‹æ´’æ´—æ´™æ´›æ´žæ´¥æ´§æ´¨æ´ªæ´«æ´®æ´±æ´²æ´µæ´¸æ´¹æ´ºæ´»æ´½æ´¾æµæµ„æµ…æµˆæµ‹æµŽæµ“æµšæµœæµ æµ£æµ¦æµ©æµªæµ¬æµ®æµ´æµ·æµ¸æ¶æ¶‚æ¶…æ¶ˆæ¶Œæ¶“æ¶”æ¶™æ¶›æ¶žæ¶ æ¶¦æ¶©æ¶¯æ¶²æ¶³æ¶µæ¶¼æ·€æ·‚æ·‡æ·‹æ·‘æ·“æ·–æ·˜æ·™æ·šæ·žæ·¡æ·£æ·¨æ·©æ·ªæ·®æ·¯æ·±æ·³æ·µæ·¸æ·ºæ·»æ·¼æ¸ƒæ¸…æ¸ˆæ¸‰æ¸Šæ¸‹æ¸“æ¸•æ¸šæ¸æ¸ æ¸¡æ¸£æ¸¥æ¸¦æ¸©æ¸¬æ¸­æ¸¯æ¸´æ¸¸æ¸ºæ¸¼æ¸¾æ¹„æ¹Šæ¹‹æ¹æ¹–æ¹˜æ¹šæ¹›æ¹§æ¹«æ¹¬æ¹®æ¹¯æ¹¾æ¹¿æº€æºˆæºæº–æºœæºæºŸæº¢æº¥æº¦æº§æºªæº«æº¯æº±æº¶æ»„æ»‡æ»‰æ»‹æ»æ»‘æ»”æ»•æ»™æ»æ»¡æ»¢æ»¨æ»©æ»´æ»¸æ»¿æ¼æ¼‚æ¼†æ¼‰æ¼Žæ¼æ¼“æ¼”æ¼ æ¼¢æ¼£æ¼©æ¼ªæ¼«æ¼¬æ¼¸æ¼¾æ¼¿æ½‡æ½‹æ½‘æ½“æ½”æ½˜æ½›æ½œæ½Ÿæ½¤æ½­æ½®æ½ºæ½¼æ½¾æ¾æ¾„æ¾Œæ¾æ¾Žæ¾æ¾”æ¾œæ¾¡æ¾¤æ¾§æ¾ªæ¾°æ¾±æ¾³æ¾¹æ¿€æ¿ƒæ¿‘æ¿•æ¿Ÿæ¿ æ¿¡æ¿¤æ¿«æ¿¬æ¿®æ¿°æ¿±æ¿µæ¿ºç€…ç€ç€šç€›ç€Ÿç€§ç€¨ç€¬ç€¾çŒçç‘ç˜çœçç£ç«ç¬ç¯ç°çµç¸ç¼ç½ç¿ç‚‰ç‚Žç‚’ç‚–ç‚™ç‚œç‚«ç‚­ç‚®ç‚³ç‚¸ç‚¹ç‚ºç‚¼ç‚½çƒ‚çƒˆçƒçƒ˜çƒ›çƒçƒŸçƒ¤çƒ¦çƒ¨çƒ­çƒ¹çƒ½ç„‰ç„Šç„•ç„—ç„™ç„šç„œç„¡ç„¦ç„¯ç„°ç„±ç„¶ç„»ç„¼ç…‡ç…‰ç…Šç…Œç…’ç…•ç…™ç…œç…žç…¥ç…¦ç…§ç…©ç…¬ç…®ç…±ç…µç†„ç†ˆç†Šç†”ç†™ç†Ÿç† ç†¨ç†­ç†±ç†¹ç†½ç†¾ç‡ç‡ƒç‡„ç‡ˆç‡Šç‡Žç‡ç‡’ç‡•ç‡šç‡Ÿç‡¦ç‡­ç‡¾ç‡¿çˆ†çˆçˆ›çˆ§çˆªçˆ­çˆ°çˆ±çˆµçˆ¶çˆ·çˆ¸çˆ¹çˆºçˆ»çˆ½çˆ¾çˆ¿ç‰†ç‰‡ç‰ˆç‰Œç‰™ç‰›ç‰Ÿç‰¡ç‰§ç‰©ç‰µç‰¹ç‰½çŠ€çŠ¬çŠ¶çŠ½ç‹€ç‹‚ç‹„ç‹ç‹—ç‹™ç‹›ç‹¡ç‹©ç‹¬ç‹­ç‹®ç‹±ç‹¸ç‹¼çŒŽçŒ›çŒœçŒŸçŒ¡çŒ¥çŒ©çŒªçŒ«çŒ¬çŒ®çŒ´çŒ¶çŒ·çŒ¿ç„ç…çŽç ç£ç§ç¨ç²çµç¸ç»çŽ„çŽ‡çŽ‰çŽ‹çŽŽçŽ–çŽ—çŽ›çŽŸçŽ¡çŽ¢çŽ¥çŽ¦çŽ¨çŽ©çŽ«çŽ®çŽ¯çŽ°çŽ²çŽ¹çŽºçŽ»ç€ç‚ç…çˆç‰çŠççç‘çžç ç£ç©ç­ç®ç³çºç½ç¾ççƒç†ç‡ç‰ç›çç¡ç¢ç¤ç¥ç¦ç¨çªç¬ç®ç°ç²ç³ç´çµç¼ç¿ç‘€ç‘‚ç‘„ç‘‹ç‘šç‘›ç‘œç‘žç‘Ÿç‘ ç‘¤ç‘©ç‘ªç‘°ç‘±ç‘³ç‘¶ç‘¾ç’€ç’ç’ƒç’‡ç’‹ç’ç’“ç’˜ç’Ÿç’¢ç’£ç’§ç’¨ç’°ç’¸ç’½ç“…ç“Šç“ç“”ç“œç“¢ç“¦ç“¶ç“·ç”„ç”Œç”•ç”˜ç”šç”œç”Ÿç”¢ç”£ç”¨ç”©ç”«ç”°ç”±ç”²ç”³ç”´ç”µç”·ç”¸ç”ºç”»ç•ˆç•Œç•‘ç•”ç•™ç•ç• ç•¢ç•¥ç•¦ç•ªç•«ç•¯ç•°ç•±ç•³ç•¶ç•¹ç–‡ç–‹ç–ç–ç–‘ç–—ç–«ç–¯ç–²ç–¼ç–¾ç—”ç—•ç—›ç—žç—§ç—´ç—¹ç—ºç˜‹ç˜¦ç™‚ç™’ç™¡ç™«ç™²ç™¸ç™ºç™»ç™¼ç™½ç™¾çš„çš†çš‡çšŽçšçš“çš•çš›çš®çš¿ç›ƒç›…ç›†ç›ˆç›Šç›Žç›ç›ç›‘ç›’ç›–ç›—ç›˜ç››ç›œç›Ÿç› ç›¡ç›£ç›¤ç›§ç›®ç›¯ç›²ç›´ç›¸ç›¼ç›¾çœçœ‰çœ‹çœŒçœžçœŸçœ çœ·çœ¼çœ¾ç€ç›çœç¡ç£ç¦çªç«ç¿çžŽçž¥çž¬çž­çž³çž¿çŸšçŸœçŸ¢çŸ¥çŸ©çŸ«çŸ­çŸ®çŸ³çŸ¶çŸ¿ç ç ‚ç ”ç •ç ¥ç °ç ´ç µç ¸ç¡•ç¡ç¡¬ç¡®ç¡²ç¡´ç¢Œç¢ç¢Žç¢‘ç¢“ç¢•ç¢—ç¢§ç¢©ç¢¹ç¢ºç¢¼ç£ç£Šç£ç£—ç£šç£¡ç£§ç£¨ç£¯ç¤ç¤’ç¤ºç¤¼ç¤¾ç¥ç¥‡ç¥ˆç¥‰ç¥ç¥–ç¥šç¥›ç¥œç¥ç¥žç¥Ÿç¥¢ç¥¤ç¥¥ç¥¨ç¥­ç¥¯ç¥·ç¥ºç¥¿ç¦ç¦„ç¦…ç¦Žç¦ç¦¤ç¦§ç¦ªç¦®ç¦°ç¦¹ç¦ºç¦»ç¦¾ç¦¿ç§€ç§ç§‰ç§‹ç§ç§‘ç§’ç§˜ç§ç§Ÿç§£ç§¤ç§¦ç§¯ç§°ç§»ç¨€ç¨‹ç¨Žç¨”ç¨—ç¨™ç¨šç¨œç¨®ç¨±ç¨²ç¨»ç¨¼ç¨½ç¨¿ç©€ç©‚ç©…ç©†ç©ç©Žç©ç©ç©—ç©£ç©©ç©´ç©¶ç©¹ç©ºç©¿çªçªˆçª—çªçª©çªªçª¯ç«…ç«ˆç«‹ç«™ç«œç«žç«Ÿç« ç«£ç«¥ç«¯ç«¶ç«¹ç«ºç¬…ç¬†ç¬‘ç¬”ç¬˜ç¬™ç¬›ç¬ ç¬¦ç¬¨ç¬¬ç¬³ç¬¹ç­†ç­ˆç­‰ç­‹ç­ç­‘ç­’ç­”ç­–ç­›ç­ç­ ç­©ç­¬ç­±ç­µç­½ç®€ç®†ç®‡ç®ç®•ç®–ç®—ç®œç®¡ç®­ç®±ç®¸ç¯€ç¯„ç¯‡ç¯‰ç¯ç¯ ç¯¤ç¯¥ç¯­ç°‘ç°—ç°¡ç°¾ç°¿ç±ƒç±Œç±ç±ç±ç±”ç±Ÿç± ç±£ç±¬ç±²ç±³ç±½ç±¾ç²‰ç²‹ç²’ç²•ç²—ç²˜ç²›ç²Ÿç²¥ç²¦ç²§ç²®ç²±ç²µç²¹ç²¾ç³€ç³Šç³•ç³–ç³Ÿç³ ç³§ç³¯ç³¸ç³»ç´€ç´„ç´…ç´‹ç´ç´ç´“ç´”ç´—ç´˜ç´™ç´šç´›ç´ ç´¡ç´¢ç´§ç´«ç´¯ç´°ç´³ç´¶ç´¸ç´¹ç´ºçµ‚çµƒçµ„çµ†çµ‹çµŒçµçµ•çµ›çµœçµ¡çµ¢çµ¦çµ®çµ±çµ²çµ³çµµçµ¶çµ¹ç¶‰ç¶“ç¶•ç¶™ç¶šç¶œç¶ ç¶¢ç¶£ç¶¨ç¶ªç¶­ç¶±ç¶²ç¶´ç¶µç¶¸ç¶ºç¶½ç¶¾ç¶¿ç·Šç·‹ç·ç·‘ç·’ç·šç·£ç·¨ç·©ç·¯ç·²ç·´ç·¹ç·»ç¸ç¸„ç¸ˆç¸“ç¸›ç¸¢ç¸£ç¸«ç¸±ç¸¹ç¸½ç¹ç¹ƒç¹†ç¹”ç¹¡ç¹£ç¹©ç¹ªç¹­ç¹½ç¹¾çºˆçºŒçºçº–çºžçº¢çº¤çº¦çº§çºªçº¬çº­çº¯çº±çº²çº³çºµçº¶çº·çº¸çº¹çº½çº¾çº¿ç»ƒç»„ç»…ç»†ç»‡ç»ˆç»ç»ç»’ç»“ç»˜ç»™ç»šç»œç»ç»Ÿç»£ç»¥ç»§ç»©ç»ªç»«ç»­ç»®ç»¯ç»´ç»µç»¼ç»¿ç¼‡ç¼ˆç¼‹ç¼Œç¼ç¼“ç¼”ç¼˜ç¼¤ç¼¥ç¼¶ç¼ºç½…ç½ç½’ç½“ç½•ç½—ç½šç½¡ç½¢ç½ªç½®ç½°ç½µç½·ç¾€ç¾…ç¾ˆç¾Šç¾Žç¾šç¾¡ç¾¢ç¾¤ç¾¨ç¾©ç¾²ç¾½ç¾¿ç¿ç¿…ç¿Šç¿Œç¿Žç¿’ç¿”ç¿˜ç¿Ÿç¿ ç¿¡ç¿©ç¿«ç¿°ç¿±ç¿²ç¿¹ç¿»ç¿¼ç¿¾è€€è€è€ƒè€…è€†è€Œè€è€•è€—è€˜è€œè€¨è€³è€¶è€½è€¿è‚è†èŠè”è–è˜èšèžè¡èªè¯è°è²è¶è·è½è¾è¿è‚è‚ƒè‚†è‚‡è‚‰è‚Œè‚–è‚˜è‚šè‚›è‚è‚ è‚¡è‚¤è‚¥è‚©è‚ªè‚¯è‚²èƒƒèƒ†èƒŒèƒŽèƒ–èƒœèƒžèƒ¡èƒ¤èƒ¥èƒ­èƒ¶èƒ¸èƒ½è„‚è„…è„†è„‡è„ˆè„‰è„‘è„–è„šè„©è„¸è„¹è„¾è…Šè…•è…¥è…¦è…©è…®è…°è…±è…¸è…¹è…¾è…¿è†€è†è†“è†šè†œè†è† è†³è†½è‡‚è‡‰è‡–è‡£è‡¥è‡§è‡¨è‡ªè‡­è‡³è‡´è‡ºè‡»è‡¼èˆ…èˆ‡èˆˆèˆŠèˆŒèˆèˆŽèˆ’èˆ–èˆ˜èˆ›èˆœèˆžèˆŸèˆ©èˆªèˆ¬èˆ¶èˆ¹è‰‡è‰šè‰¦è‰¯è‰²è‰³è‰¶è‰·è‰¸è‰¹è‰ºè‰¾èŠ‚èŠƒèŠŠèŠ‹èŠ‘èŠ’èŠ“èŠ™èŠèŠ èŠ£èŠ¥èŠ¦èŠ©èŠ«èŠ¬èŠ­èŠ®èŠ¯èŠ±èŠ³èŠ´èŠ·èŠ¸èŠ¹èŠ½è‹…è‹‡è‹è‹è‹‘è‹“è‹•è‹—è‹›è‹Ÿè‹ è‹¡è‹¥è‹¦è‹§è‹«è‹¯è‹±è‹¹è‹ºè‹¾èŒèŒ‚èŒƒèŒ„èŒ…èŒ‰èŒ”èŒ—èŒ›èŒœèŒ¤èŒ¨èŒ«èŒ±èŒ²èŒ³èŒ´èŒµèŒ¶èŒ¸èŒ¹èƒè†è‰èŠèèè’è”è˜èŸè¡è£è­è³è·è»è¼èŽ‰èŽŠèŽŽèŽ“èŽ˜èŽ™èŽ›èŽœèŽ£èŽ¨èŽªèŽ«èŽ±èŽ²èŽ³èŽ·èŽ¹èŽ¼è€èè…è‡èˆèŠèŒèè“è–èœèžèŸè è¡è©è¯è°è±è²è¸è»è½è¿è„èŠèŒèè“èè è¢è¤è¥è§è¨è©è¬è±è½è‘†è‘‰è‘‘è‘—è‘›è‘¡è‘£è‘¦è‘«è‘¬è‘­è‘±è‘³è‘µè‘¶è‘ºè’‚è’‹è’”è’™è’œè’žè’£è’¨è’²è’¸è’¹è’»è’¼è’½è“è“‰è“‹è“è“‘è““è“™è“œè“è“¥è“¬è“®è“¼è”†è”“è”—è”šè”Ÿè”¡è”£è”¥è”¦è”§è”­è”µè”ºè”»è”¼è•ƒè•‰è•Šè•Žè•—è•™è•¨è•«è•­è•²è•´è•·è•¾è–„è–†è–‡è–ˆè–è–‘è–”è–—è–™è–›è–œè–¡è–¦è–©è–ªè–«è–¬è–®è–¯è–°è—è—ƒè—è—è—•è—è—¤è—¥è—©è—ªè—°è—»è˜†è˜‡è˜Šè˜‹è˜è˜‘è˜“è˜”è˜­è˜¾è˜¿è™„è™Žè™è™‘è™”è™•è™šè™›è™žè™Ÿè™«è™¹è™¾èšèš‚èšŠèšªèšµè›‡è›‹è›è›™è›Ÿè›¤è›­è›®è›¯èœ€èœ‚èœƒèœŠèœ“èœšèœœèœ¡èœ¢èœ¥èœ·èœ»è€èƒè‰èŒèŽè™èŸè è¦èªè´è¶è¸èž‚èžƒèžèžžèž¢èžºèŸ†èŸ‘èŸ²èŸ·èŸ¹èŸ»è è ”è Ÿè ¡è ¢è »è¡€è¡…è¡†è¡Œè¡è¡“è¡—è¡›è¡è¡žè¡¡è¡£è¡¥è¡¨è¡«è¡¬è¡®è¡°è¡¿è¢è¢†è¢ˆè¢‹è¢“è¢–è¢«è¢´è£è£‚è£ƒè£…è£è£”è£•è£œè£è£Ÿè£¡è£¬è£³è£´è£¹è£½è£¾è¤‡è¤Œè¤è¤“è¤šè¤²è¥Ÿè¥ªè¥²è¥¿è¦€è¦è¦ƒè¦…è¦‡è¦‹è¦è¦–è¦šè¦§è¦ªè¦³è¦ºè§€è§è§‚è§„è§…è§†è§ˆè§‰è§’è§œè§£è§¦è¨€è¨‚è¨ˆè¨Šè¨Žè¨“è¨—è¨˜è¨©è¨ªè¨­è¨±è¨´è¨¶è¨¼è©è©”è©•è©žè© è©¡è©¦è©©è©®è©°è©±è©³è©¹è©ºèª…èª‡èª‰èªŒèªèª’èª“èª˜èªžèª èªªèª¬èª­èª°èª²èª¼èª¿è«„è«‡è«‹è«Œè«è«’è«–è« è«¦è««è«­è«®è«±è«µè«¸è«ºè«¾è¬€è¬‚è¬„è¬Žè¬™è¬šè¬›è¬è¬´è¬¹è­‰è­˜è­™è­šè­œè­¦è­°è­²è­·è­½è­¿è®€è®ƒè®Šè®è®“è®šè®¡è®¢è®¨è®©è®­è®¯è®°è®²è®·è®¸è®ºè®¿è¯€è¯è¯„è¯†è¯‰è¯Šè¯è¯‘è¯’è¯•è¯—è¯šè¯è¯¥è¯¦è¯­è¯±è¯´è¯¶è¯·è¯¸è¯ºè¯»è°ƒè°†è°ˆè°Šè°Žè°è°™è°¢è°¦è°¨è°­è°·è±†è±ˆè±‰è±Šè±Žè±è±”è±•è±šè±¡è±ªè±«è±¬è±­è±¹è±ºè²…è²“è²”è²è²žè² è²¡è²¢è²§è²©è²«è²®è²³è²´è²·è²¸è²»è²½è²¿è³€è³ƒè³‡è³ˆè³Šè³“è³›è³œè³žè³ è³¢è³£è³¦è³ªè³´è³¼è³½è´€è´„è´ˆè´è´žè´¡è´¢è´¤è´¦è´­è´°è´±è´µè´¸è´¹è´ºè´»è´¼è´¾èµ„èµˆèµ‹èµèµ“èµ–èµ›èµžèµ¢èµ¤èµ¦èµ«èµ°èµ³èµ´èµµèµ·è¶…è¶Šè¶™è¶£è¶³è¶´è·ƒè·‹è·Œè·‘è·è·Ÿè·¡è·¨è·©è·¯è·³è¸è¸žè¸©è¸ªè¹ˆè¹Ÿè¹¤è¹¦èº…èºèº‘èº«èººè»Šè»Œè»è»’è»Ÿè»¢è»¸è»»è»½è¼ƒè¼‰è¼Šè¼”è¼•è¼è¼©è¼ªè¼¸è¼¿è½…è½‰è½Žè½Ÿè½¦è½¨è½©è½¬è½®è½¯è½¶è½»è¾…è¾ˆè¾‰è¾“è¾•è¾›è¾œè¾žè¾£è¾­è¾°è¾²è¾¹è¾ºè¾»è¾¼è¾¾è¾¿è¿è¿…è¿‡è¿ˆè¿Žè¿è¿‘è¿”è¿˜è¿›è¿œè¿è¿žè¿Ÿè¿¦è¿©è¿ªè¿«è¿­è¿°è¿´è¿·è¿¹è¿ºè¿½é€€é€é€‚é€ƒé€…é€†é€‰é€Šé€é€é€é€‘é€”é€–é€—é€™é€šé€é€Ÿé€ é€¢é€£é€±é€²é€¸é€ºé€¼é€½é‚é„é…é‡éŠé‹éŽéé“é”é•é—é™éœéžé é¢é£é¥é©é²é´é¸éºé¼é¿é‚é‚‚é‚„é‚‰é‚Šé‚‹é‚é‚‘é‚“é‚é‚¢é‚£é‚¦é‚¨é‚ªé‚¬é‚±é‚µé‚¹é‚»éƒéƒŽéƒ‘éƒéƒžéƒ¡éƒ¨éƒ­éƒµéƒ·éƒºéƒ½éƒ¿é„‰é„’é„”é„žé„¥é„§é„­é„ºé…ƒé…‰é…Šé…‹é…é…’é…”é…¢é…¥é…©é…¬é…µé…·é…¸é†‰é†‹é†é†é†’é†œé†«é†¬é†¸é†ºé‡†é‡‡é‡ˆé‡‰é‡Šé‡‹é‡Œé‡é‡Žé‡é‡‘é‡˜é‡œé‡é‡¡é‡£é‡§é‡¨é‡¼éˆéˆéˆ‘éˆ•éˆžéˆ£éˆ¦éˆ®éˆ´éˆºéˆ¿é‰ƒé‰„é‰…é‰ˆé‰›é‰¢é‰±é‰¾éŠ€éŠ…éŠ‘éŠ“éŠ•éŠ˜éŠœéŠ­éŠ®éŠ³é‹†é‹é‹’é‹¤é‹ªé‹­é‹°é‹³é‹¼éŒ„éŒ‹éŒéŒ’éŒ•éŒšéŒ éŒ¡éŒ¢éŒ¦éŒ«éŒ¬éŒ¯éŒ²éŒ¶é†é‡éŠé‹é›éœé¬éµé¶é¾éŽ‚éŽ‡éŽŠéŽŒéŽ”éŽ–éŽ›éŽ§éŽ¬éŽ®é†éˆé‹é‘é—éžéŸé¡éµé»é˜é™éµé‘’é‘“é‘£é‘«é‘½é’ˆé’Šé’•é’šé’œé’Ÿé’¡é’¢é’¥é’¦é’§é’®é’°é’±é’²é’¶é’¸é’»é“é“ƒé“Žé“›é“ é“¤é“§é“¨é“­é“®é“µé“¶é“ºé“¾é”€é”é”…é”‹é”Œé”Žé”é”—é”é”Ÿé” é”£é”¤é”¦é”«é”®é”²é”´é”¶é”ºé•é•…é•‡é•’é•”é•–é•œé•«é•­é•±é•·é•¿é–€é–ƒé–‰é–‹é–‘é–’é–“é–”é–˜é–™é–¢é–£é–¬é–²é–»é—†é—‡é—Šé—’é—˜é—œé—¨é—ªé—­é—®é—¯é—²é—´é—·é—¹é—»é˜€é˜é˜…é˜Œé˜Žé˜‘é˜—é˜™é˜šé˜œé˜é˜Ÿé˜¡é˜ªé˜®é˜²é˜³é˜´é˜¿é™€é™„é™…é™†é™ˆé™Œé™é™é™žé™¢é™£é™¤é™¨é™©é™ªé™°é™³é™µé™¶é™·é™¸é™ºé™½éš…éš†éšˆéšŠéš‹éšŽéšéšéš”éš›éš éš£éš§éš¨éšªéš±éš³éš´éš·éš¹éš»éš¼éš½éš¾éš¿é›€é›é›„é›…é›†é›‡é›‰é›‹é›Œé›é›é›‘é›•é›™é››é›œé›žé›¢é›£é›¨é›ªé›¯é›²é›¶é›·é›»é›¾éœ€éœéœ‚éœ„éœ†éœ‡éœˆéœ‰éœŠéœéœéœ“éœ–éœœéœžéœ§éœ­éœ²éœ¸éœ¹é‚é„éˆé‘é’é“é–é™éšéœéžé¢é©é­é³é´éž‹éžéž˜éž éž¥éŸ‹éŸ“éŸ¦éŸ©éŸ«éŸ¬éŸ®éŸ³éŸµéŸ¶éŸ»éŸ¿é é ‚é ƒé †é ˆé Œé é ’é “é ”é —é ˜é ¡é ¤é ¨é ¬é ­é °é ´é ¹é »é ¼é¡†é¡Œé¡é¡é¡“é¡”é¡•é¡˜é¡›é¡žé¡§é¡¯é¡µé¡¶é¡ºé¡½é¡¾é¡¿é¢‚é¢„é¢†é¢‰é¢é¢é¢‘é¢–é¢˜é¢œé¢é¢ é¢¨é¢¯é£„é£†é£Žé£é£•é£˜é£šé£›é£žé£Ÿé£ é£¢é£¯é£²é£´é£¼é£½é£¾é¤ƒé¤…é¤Šé¤Œé¤é¤’é¤“é¤˜é¤¨é¥…é¥’é¥—é¥ªé¥­é¥®é¥°é¥ºé¥¼é¦–é¦™é¦œé¦¥é¦¨é¦¬é¦®é¦³é¦´é¦·é§é§„é§†é§’é§•é§é§±é§¿é¨Žé¨é¨“é¨¦é¨«é¨®é¨°é¨·é©€é©Šé©•é©—é©šé©›é©¢é©¬é©°é©´é©·é©»é©¾é©¿éª„éª†éªéª‘éª—éª¨é«”é«˜é«™é«ªé«­é«®é¬ƒé¬†é¬é¬šé¬¥é¬®é¬±é¬¼é­é­‚é­ƒé­„é­…é­‡é­ˆé­‰é­é­é­‘é­”é­šé­¯é­°é­·é®Žé®‘é®’é®¨é®ªé®«é®®é¯‰é¯Šé¯›é¯¨é°‚é°é°»é±—é±»é±¼é²é²é²œé²¨é²¸é³žé³¥é³©é³¯é³²é³³é³´é´‡é´‰é´¨é´©é´«é´»é´¾é´¿éµœéµéµ¤éµ¬éµ²éµ½é¶é¶‘é¶´é·„é·‡é·—é·²é·¹é·ºé¸žé¸Ÿé¸¡é¸¢é¸£é¸¥é¸¦é¸­é¸½é¸¾é¸¿é¹‚é¹…é¹Œé¹é¹‘é¹¤é¹°é¹½é¹¿éº‚éºˆéº’éº“éº—éºŸéº¥éº¦éºµéººéº»éº¼éº½éº¿é»ƒé»„é»é»Žé»‘é»’é»”é»˜é»™é»›é»žé»Ÿé»¨é¼ˆé¼Žé¼é¼“é¼ é¼»é¼¾é½Šé½‹é½é½¡é½¢é½ªé½®é½·é¾„é¾é¾é¾”é¾˜é¾™é¾šé¾œé¾Ÿê€€ê€†ê€‡ê€ˆê€Šê€‹ê€Žê€’ê€“ê€˜ê€¡ê€¤ê€§ê€ªê€­ê€·ê€¸ê…êŠêŒêê•ê£ê§ê±ê²ê´ê»ê‚…ê‚‘ê‚–ê‚šê‚ ê‚¦ê‚µêƒ…êƒ†êƒ”êƒ©êƒ²êƒ¼ê„™ê„žê„Ÿê…ê…ê…¤ê†‚ê†¨ê†°ê†¹ê‡ƒê‡Žê‡–ê‡©êˆšêˆ›êˆ¤êˆµêˆ¼ê‰„ê‰“ê‰£êŠêŠ›êŠ¼ê‹Šê‹’ê‹–ê‹ªê‹«ê‹¬êŒƒêŒ…êŒ—êŒšêŒ¦êŒ©êˆêŒêê›êŸê©êŽ‡êŽ­ê‚ê„êˆêê¤ê¹ê¿êˆêžê‘€ê‘˜ê’Œê“ê“‘ê“’ê““ê“”ê“•ê“–ê“—ê“˜ê“™ê“šê“›ê“ê“žê“Ÿê“ ê“¡ê“¢ê“£ê“¤ê“¥ê“¦ê“§ê“¨ê“©ê“ªê“«ê“¬ê“­ê“®ê“¯ê“°ê“±ê“²ê“³ê“´ê“µê“¶ê“·ê“¸ê“¹ê“½ê”€ê”ê”…ê”‰ê”Šê”‹ê””ê”–ê”˜ê”šê”œê”ê”žê” ê”¡ê”ªê”¬ê”­ê”®ê”°ê”·ê”¹ê”ºê”»ê”½ê•‚ê•”ê••ê•—ê•˜ê•¤ê•¥ê•¬ê•­ê•¯ê•·ê•¹ê•ºê•»ê–€ê–ê–’ê–˜ê–›ê–œê–žê–§ê–¯ê–²ê–´ê–µê–¸ê–¹ê–»ê–¼ê–½ê—ƒê—„ê—ˆê—‹ê—ê—›ê—œê—ê—žê—Ÿê—£ê—¥ê—®ê—¯ê—°ê—´ê—¶ê˜‚ê˜ˆê˜Œê˜ê˜‘ê˜’ê˜–ê˜˜ê˜œê˜ê™¨ê™®ê™°ê™²êœ­ê„ê…ê«ê®ê¯êž€êžêžªêž«êž¬êž®êž°êž±êž²ê €ê ‡ê Žê –ê ê žê ¡ê £ê ¤ê ¥ê ¦ê¡°ê¤‹ê¤Žê¤šê¤¢ê¤§ê¤¨ê¤¬ê¤­ê¤¶ê¦„ê¦†ê¦Œê¦ê¦’ê¦”ê¦•ê¦šê¦ ê¦¤ê¦¥ê¦¦ê¦©ê¦ªê¦«ê¦®ê¦±ê¦²ê¦³ê¦´ê¦¶ê¦¸ê¦ºê¦¿ê§€ê§ê§†ê§¼ê§¾êª‹êª”êª–êªœêª²êª³ê«›ê«ê«žê«ªê­°ê­±ê­²ê­³ê­´ê­µê­¶ê­·ê­¸ê­¹ê­ºê­»ê­¼ê­½ê­¾ê­¿ê®€ê®ê®‚ê®ƒê®„ê®…ê®†ê®‡ê®ˆê®‰ê®Šê®‹ê®ê®Žê®ê®ê®‘ê®’ê®“ê®”ê®•ê®–ê®—ê®˜ê®™ê®šê®›ê®œê®ê®žê®Ÿê® ê®¡ê®¢ê®£ê®¤ê®¥ê®¦ê®§ê®¨ê®©ê®ªê®«ê®¬ê®­ê®®ê®¯ê®°ê®±ê®²ê®³ê®´ê®µê®¶ê®·ê®¸ê®¹ê®ºê®»ê®¼ê®½ê®¾ê®¿ê¯€ê¯ê¯‚ê¯ƒê¯„ê¯…ê¯†ê¯‡ê¯ˆê¯‰ê¯Šê¯‹ê¯Œê¯ê¯Žê¯ê¯ê¯‘ê¯’ê¯“ê¯”ê¯•ê¯–ê¯—ê¯˜ê¯šê¯œê¯ê¯žê¯Ÿê¯ ê¯¡ê¯¢ê¯£ê¯¤ê¯¥ê¯¦ê¯§ê¯¨ê¯©ê¯ªê¯­ê°€ê°„ê°ˆê°ê°‘ê°•ê°™ê°œê° ê°¬ê°¯ê°¸ê±°ê±´ê±¸ê²€ê²‰ê²Œê²ê²”ê²œê² ê²¨ê²©ê²°ê²¸ê²½ê³„ê³ ê³¡ê³¤ê³°ê³³ê³µê³¼ê³½ê´€ê´‚ê´‘êµêµ¬êµ­êµ°êµ´ê¶ê¶‡ê¶Œê·€ê·œê· ê·¸ê·¹ê·¼ê¸€ê¸ˆê¸°ê¸´ê¸¸ê¹€ê¹ƒê¹Šê¹ê¹œêº¼ê»˜ê»«ê¼¬ê¼´ê½ƒê¾¼ê¿ˆë„ëŒë‚˜ë‚™ë‚œë‚Ÿë‚ ë‚¨ë‚©ë‚­ë‚³ë‚´ë‚¸ë‚¼ëƒ‡ëƒ‰ëƒëƒ”ë„ˆë„›ë„¤ë„¨ë„¬ë„´ë„·ë…€ë…„ë…ˆë…•ë…¸ë…¼ë…¿ë†€ë†ˆë†‹ë†ë†ë†’ë†“ë‡Œë‡¨ë‡¸ëˆ„ëˆˆëˆŒëŠëŠ”ëŠ˜ëŠ™ëŠ¡ëŠ¬ë‹ˆë‹‰ë‹Œë‹ë‹˜ë‹›ë‹œë‹¤ë‹¨ë‹«ë‹¬ë‹´ë‹¹ë‹½ëŒ€ëŒëŒ„ëŒëŒœëŒ ë”ë•ëŸë°ë±ë¸ëŽŒë„ë…ëˆëŒë—ë™ë ë‘ë‘”ë‘˜ë‘ ë‘¥ë“€ë“—ë“œë“ë“ ë“£ë“¤ë”ˆë””ë”•ë”˜ë”œë”§ë”©ë”´ë– ë–°ë–¼ë˜ë˜‘ëš±ëœ¸ë ë°ë¼ë½ëž€ëž‚ëž„ëž…ëžŒëžëžëž‘ëž’ëž”ëž—ëž˜ëžœëžŸëž ëž«ëž­ëž°ëž´ëž¼ëŸ„ëŸ‰ëŸ¬ëŸ­ëŸ°ëŸ¼ë ë ‚ë ˆë ‰ë Œë ë ˜ë ›ë œë £ë ¤ë ¥ë ¨ë ¹ë¡€ë¡œë¡ë¡ ë¡¬ë¡¯ë¡±ë¡²ë¡¸ë£Œë£¡ë£¨ë£©ë£¬ë£°ë£½ë£¾ë¤ ë¥˜ë¥œë¥ ë¥­ë¥´ë¥¸ë¥¼ë¦„ë¦‰ë¦ë¦ë¦¬ë¦­ë¦°ë¦³ë¦´ë¦¼ë¦½ë¦¿ë§ë§‚ë§†ë§ˆë§‰ë§Œë§Žë§ë§ë§˜ë§›ë§ë§žë§ ë§¡ë§£ë§¤ë§¥ë§¨ë§¹ë§ºë¨€ë¨•ë¨¸ë¨¼ë¨¿ë©‹ë©ë©“ë©”ë©˜ë©œë©´ë©¶ëª…ëª¨ëª©ëªªëª¬ëª¯ëª°ëª¹ëª»ëª½ë«€ë«¼ë¬˜ë¬¨ë¬­ë¬²ë¬³ë¬´ë¬µë¬¶ë¬¸ë¬»ë­ë¯€ë¯¤ë¯¸ë¯¹ë¯¼ë¯¿ë°€ë°‹ë°ë°‘ë°”ë°•ë°˜ë°›ë°œë°§ë°©ë°¯ë°°ë°±ë°´ë°¸ë±…ë²„ë²ˆë²Œë²”ë²•ë²—ë²™ë²šë² ë²¤ë²¨ë²³ë³€ë³„ë³‘ë³´ë³µë³¸ë´„ë´…ë´‰ë´ë¶€ë¶ë¶„ë¶ˆë¶“ë¶•ë¸Œë¸ë¸ë¸”ë¸§ë¹„ë¹…ë¹†ë¹ˆë¹Œë¹›ë¹¨ë¿Œë¿ë¿”ì˜ìœì¨ì‚¬ì‚°ì‚´ì‚¼ìƒ€ìƒìƒ…ìƒˆìƒ‰ìƒŒìƒìƒ˜ìƒ›ìƒìƒ¤ìƒ¬ìƒ´ìƒ¹ì„œì„ì„ ì„¤ì„­ì„¯ì„±ì„¸ì„¹ì„¼ì…€ì…ˆì…‰ì…”ì…œì…°ì…¸ì†Œì†ì†ì†”ì†¡ì†§ì†¨ì‡¼ì‡½ìˆ€ìˆ’ìˆ“ìˆ˜ìˆ™ìˆœìˆ ìˆ©ìˆ­ìˆ¸ì‰ìŠˆìŠìŠ¤ìŠ¥ìŠ¨ìŠ¬ìŠ´ìŠ¹ì‹œì‹ì‹žì‹ ì‹¤ì‹«ì‹¬ì‹­ì‹¶ì‹¸ìŒìŒ°ì¨ì¬ì½ìŽŒì‘¹ì“°ì”¨ì”¬ì”¸ì”½ì•„ì•…ì•ˆì•‰ì•‹ì•Œì•”ì••ì•—ì•™ì•žì•Ÿì• ì•¡ì•¤ì•¨ì•°ì•¼ì•½ì–€ì–Œì–ì–ì–‘ì–—ì–˜ì–œì–²ì–´ì–¸ì–¼ì—„ì—…ì—†ì—ˆì—‰ì—ì—ì—”ì—˜ì— ì—¦ì—©ì—¬ì—­ì—°ì—´ì˜ì˜ˆì˜ì˜ì˜¤ì˜¥ì˜¨ì˜«ì˜¬ì˜´ì˜¹ì˜¿ì™€ì™„ì™ˆì™‘ì™“ì™”ì™•ì™¸ì™¼ìš”ìš•ìš˜ìšœìš©ìš°ìš±ìš´ìš·ìš¸ì›€ì›ƒì›…ì›Œì›ì›”ì›¡ì›¨ì›¬ìœ„ìœ…ìœˆìœŒìœ ìœ¡ìœ¤ìœ¨ìœ¼ì€ìƒì„ìŒì‘ì˜ì´ìµì¸ì¼ì½ìž„ìž…ìžˆìž‰ìžŒìžìž‘ìž”ìž˜ìž ìž¥ìž¬ìž­ìž¼ìž¿ìŸìŸ†ìŸˆì €ì ì „ì Šì ì “ì •ì œì  ì ¤ì ¬ì ¸ì¡°ì¡±ì¢…ì¢‹ì¢Œì£ ì£¼ì£½ì¤€ì¤„ì¤‘ì¤˜ì¥ì¥ì¥”ì¥¬ì¦ˆì¦Œì¦ì¦ì§€ì§ì§„ì§ˆì§ì§‘ì§–ì§œì§¬ì§¸ì¨”ì¨¨ìª½ì­ˆì¯”ì°Œì°¨ì°©ì°¬ì°°ì°¸ì°½ì±€ì±„ì±…ì²œì² ì²¨ì²­ì²´ì²¸ì²»ì²¼ì³‰ì³ì´ˆì´Œì´žìµœì¶”ì¶•ì¶˜ì¶œì¶©ì·¨ì¸„ì¸ ì¸¼ì¹˜ì¹œì¹ ì¹¨ì¹´ì¹¼ìº‰ìº”ìº ì»¤ì»¨ì¼€ì¼„ì¼ˆì¼“ì¼™ì¼œì¼±ì¼¸ì½”ì½œì½¤ì¿³í€´íí“í¬í°íºí‚Ší‚¤í‚¨í‚¬í‚´í‚¿íƒ€íƒíƒ„íƒˆíƒ‰íƒíƒ•íƒœíƒíƒ­íƒ±íƒ·í„°í„¸í…Œí…í…í…”í…œí†„í† í†¤í†¨í†°í†µíˆ¬íˆ´íŠœíŠ íŠ¤íŠ¬íŠ¸íŠ¼í‹€í‹°í‹´íŒ€íŒƒíŒ…íŒŒíŒíŒíŒ‘íŒ”íŒ¨íŒ©íŒ¬í¼í½íŽ€íŽ„íŽ˜íŽ íŽ´íŽ¸í‰íí¬í­í´í‘œí‘¸í’€í’ˆí’‹í’í“¨í”€í”„í”ˆí”Œí”¼í•„í•í•‘í•˜í•™í•œí•Ÿí• í•¨í•©í•«í•­í•´í•¸í–„í–‡í–ˆí–‰í–¥í—ˆí—Œí—í—˜í—¤í—¥í—¨í—¬í—·í˜€í˜í˜„í˜ˆí˜‘í˜•í˜œí˜¸í˜¼í™€í™í™”í™•í™˜í™œí™©íšŒíš¡íš¨íš¬í›„í›ˆí›Œí›¨íœ˜íœ´íœ¸í‹íí‘í”íŸí í£í¥í¬í±ížˆížŒížížíž˜ï¨‘ï¬“ï¬”ï¬•ï¬–ï¬—ï¬ ï¬¡ï¬£ï¬¦ï¬§ï¬¨ï¬­ï¬®ï¬¯ï¬³ï¬´ï¬µï¬¹ï¬¼ï¬¾ï­ï­†ï­‡ï­‰ï­Šï­Žï­ï­ï­‘ï­’ï­“ï­”ï­•ï­–ï­—ï­˜ï­™ï­šï­›ï­œï­ï­žï­Ÿï­ ï­¡ï­¢ï­£ï­¤ï­¥ï­¦ï­§ï­¨ï­©ï­ªï­«ï­¬ï­­ï­®ï­¯ï­°ï­±ï­²ï­³ï­´ï­µï­¶ï­·ï­¸ï­¹ï­ºï­»ï­¼ï­½ï­¾ï­¿ï®€ï®ï®‚ï®ƒï®„ï®…ï®†ï®‡ï®ˆï®‰ï®Šï®‹ï®Œï®ï®Žï®ï®ï®‘ï®’ï®“ï®”ï®•ï®–ï®—ï®˜ï®™ï®šï®›ï®œï®ï®žï®Ÿï® ï®¡ï®¢ï®£ï®¤ï®¥ï®¦ï®§ï®¨ï®©ï®ªï®«ï®¬ï®­ï®®ï®¯ï®°ï®±ï¯“ï¯”ï¯•ï¯–ï¯—ï¯˜ï¯™ï¯šï¯›ï¯œï¯ï¯žï¯Ÿï¯ ï¯¡ï¯¢ï¯£ï¯¤ï¯¥ï¯¦ï¯§ï¯¨ï¯©ï¯¼ï¯½ï¯¾ï¯¿ï°Žï°—ï°²ï±ï±™ï±žï±Ÿï± ï±¡ï±¢ï±¯ï²Œï²•ï²›ï²ï² ï²­ï³Šï³Œï³ï³’ï³™ï³£ï³©ï³ªï³²ï³³ï³´ï³¼ï´¼ï´½ï´¾ï´¿ï¶Šï·²ï·³ï·´ï·¶ï·¸ï·ºï·»ï¸Žï¸ï¸¶ï¸»ï¹°ï¹±ï¹²ï¹³ï¹´ï¹¶ï¹·ï¹¸ï¹¹ï¹ºï¹»ï¹¼ï¹½ï¹¾ï¹¿ïº€ïºïº‚ïºƒïº„ïº…ïº†ïº‡ïºˆïº‰ïºŠïº‹ïºŒïºïºŽïºïºïº‘ïº’ïº“ïº”ïº•ïº–ïº—ïº˜ïº™ïºšïº›ïºœïºïºžïºŸïº ïº¡ïº¢ïº£ïº¤ïº¥ïº¦ïº§ïº¨ïº©ïºªïº«ïº¬ïº­ïº®ïº¯ïº°ïº±ïº²ïº³ïº´ïºµïº¶ïº·ïº¸ïº¹ïººïº»ïº¼ïº½ïº¾ïº¿ï»€ï»ï»‚ï»ƒï»„ï»…ï»†ï»‡ï»ˆï»‰ï»Šï»‹ï»Œï»ï»Žï»ï»ï»‘ï»’ï»“ï»”ï»•ï»–ï»—ï»˜ï»™ï»šï»›ï»œï»ï»žï»Ÿï» ï»¡ï»¢ï»£ï»¤ï»¥ï»¦ï»§ï»¨ï»©ï»ªï»«ï»¬ï»­ï»®ï»¯ï»°ï»±ï»²ï»³ï»´ï»µï»¶ï»·ï»¸ï»¹ï»ºï»»ï»¼ï¼ï¼ˆï¼‰ï¼Šï¼ï¼Žï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼²ï¼³ï¼´ï¼µï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½Žï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï½žï½¡ï½¤ï½§ï½¨ï½°ï½±ï½²ï½´ï½¶ï½ºï½»ï½¼ï½½ï¾‚ï¾‰ï¾Šï¾Œï¾ï¾’ï¾•ï¾—ï¾˜ï¾™ï¾šï¾›ï¾ï¾žï¾ ï¾¶ï¿½ð“ð““ð“—ð“˜ð“šð“›ð“\n",
      "\n",
      "Target classes: South America, Western Europe, Middle Africa, Eastern Asia, Central Asia, Northern Europe, Caribbean, Western Asia, Melanesia, Southern Africa, South-eastern Asia, Western Africa, Central America, Eastern Europe, Northern Africa, Eastern Africa, Southern Asia, Northern America, Southern Europe\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import country_converter as coco\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from utils.data import NameNationalityData, NameNationalityDataStream\n",
    "from utils.model import RNN_Nationality_Predictor, Transformer_Nationality_Predictor\n",
    "from sklearn.metrics import roc_auc_score, top_k_accuracy_score, accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score, average_precision_score\n",
    "import lightning as L\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, EarlyStopping\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR, ConstantLR\n",
    "\n",
    "device: str = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# parameters\n",
    "MAXIMUM_NAME_LENGTH: int = 40 # maximum number of characters\n",
    "BATCH_SIZE: int = 2048 # number of training examples per batch\n",
    "N_EVAL: int = 100 # evaluate loss every n batches\n",
    "MAX_EPOCHS: int = 10\n",
    "EARLY_STOPPING_PATIENCE: int = 20 # checks, not steps. so a patience of 20 corresponds to 20*n_eval steps\n",
    "GRADIENT_CLIPPING_VAL: float = 1.0\n",
    "\n",
    "# hyperparameters\n",
    "ARCHITECTURE = 'LSTM' # one of 'RNN', 'GRU' or 'LSTM'\n",
    "EMBEDDING_DIM = 64 # number of dimensions of embedded tensor\n",
    "HIDDEN_SIZE = 256 # number of neurons in hidden layer of rnn\n",
    "NUM_RNN_LAYERS = 3 # number of stacked rnn layers\n",
    "DROPOUT = 0.1 # dropout probability\n",
    "\n",
    "# lr scheduler parameters\n",
    "WARMUP_STEPS = 1000 # number of steps for warmup\n",
    "COSINE_STEPS = 50000 # number of steps for cosine annealing\n",
    "MIN_LR = 4e-5 # minimum learning rate after cosine annealing ends\n",
    "MAX_LR = 6e-3 # maximum learning rate when warmup ends and cosine starts\n",
    "\n",
    "# read country codes\n",
    "with open('./data/.country_codes', 'r') as f:\n",
    "    COUNTRY_CODES: list = f.read().splitlines()\n",
    "print(f'Country codes: {\", \".join(COUNTRY_CODES)}')\n",
    "\n",
    "#read vocabulary (all unique characters used in the dataset)\n",
    "with open('./data/.vocabulary', 'r') as f:\n",
    "    VOCABULARY: str = f.read()\n",
    "print(f'Vocabulary: {VOCABULARY}')\n",
    "\n",
    "# generate country code mappings\n",
    "target_class: str = 'UNregion' # see country_converter documentation on PyPI for available classes\n",
    "COUNTRY_MAPPING: dict = {cc: coco.convert(names=cc, to=target_class) for cc in COUNTRY_CODES} \n",
    "print(f'Target classes: {\", \".join(list(set(COUNTRY_MAPPING.values())))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPORT DATA**\n",
    "\n",
    "- train.csv gets streamed in chunks\n",
    "- val.csv will be loaded into memory as a whole\n",
    "- name strings will be encoded as integer tensors where index i maps to the i-th character in the vocabulary\n",
    "- zero will be used as padding index, names longer than max_name_length will be truncated\n",
    "- the tensors will have a shape of (batch_size, max_name_length)\n",
    "- the dataset also generates a tensor of shape (batch_size) that holds the sequence length (number of characters) of the current name\n",
    "- countries will be converted to one-hot-encoded tensors of shape (batch_size, n_countries+1) where n_countries is the number of output classes in the COUNTRY_MAPPING dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NameNationalityDataStream(\n",
    "    data_file='./data/train.csv',\n",
    "    chunksize=BATCH_SIZE,\n",
    "    maximum_name_length=MAXIMUM_NAME_LENGTH,\n",
    "    vocabulary=VOCABULARY,\n",
    "    country_codes=COUNTRY_CODES,\n",
    "    country_mapping=COUNTRY_MAPPING\n",
    ")\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2441088 records.\n"
     ]
    }
   ],
   "source": [
    "val_data = NameNationalityData(\n",
    "    data_file='./data/val.csv',\n",
    "    maximum_name_length=MAXIMUM_NAME_LENGTH,\n",
    "    vocabulary=VOCABULARY,\n",
    "    country_codes=COUNTRY_CODES,\n",
    "    country_mapping=COUNTRY_MAPPING\n",
    ")\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2441088 records.\n"
     ]
    }
   ],
   "source": [
    "test_data = NameNationalityData(\n",
    "    data_file='./data/test.csv',\n",
    "    maximum_name_length=MAXIMUM_NAME_LENGTH,\n",
    "    vocabulary=VOCABULARY,\n",
    "    country_codes=COUNTRY_CODES,\n",
    "    country_mapping=COUNTRY_MAPPING\n",
    ")\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MODELING**\n",
    "\n",
    "- Create simple model using character embeddings, rnn layers and a dense layer\n",
    "- embedding layer maps input tensor of shape (batch_size, max_name_length) to embedding tensor of shape (batch_size, max_name_length, embedding_dim)\n",
    "- the embedding tensor and sequence_lengths tensor will be used to [pack a padded batch](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html), which enables variable length inputs\n",
    "- the packed sequence will be passed to the rnn layer \n",
    "- the hidden state of the last rnn layer will be used passed through a dense layer to create an output of shape (batch_size, n_countries+1), where where n_countries is the number of output classes in the COUNTRY_MAPPING dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning Wrapper\n",
    "class LightningModelWrapper(L.LightningModule):\n",
    "    def __init__(self, mlflow_logger, model, criterion):\n",
    "        super().__init__()\n",
    "        self.mlflow_logger = mlflow_logger\n",
    "\n",
    "        # log model summary and model hyperparameters\n",
    "        self.model = model\n",
    "        with open(\"model_summary.txt\", \"w\") as f:\n",
    "            f.write(str(summary(self.model)))\n",
    "        self.mlflow_logger.experiment.log_artifact(local_path=\"model_summary.txt\", run_id=self.mlflow_logger.run_id)\n",
    "        hyperparams = {\n",
    "            \"input_size\": self.model.input_size,\n",
    "            \"output_size\": self.model.output_size,\n",
    "            \"architecture\": self.model.architecture,\n",
    "            \"embedding_dim\": self.model.embedding_dim,\n",
    "            \"hidden_size\": self.model.hidden_size,\n",
    "            \"num_rnn_layers\": self.model.num_rnn_layers,\n",
    "            \"dropout\": self.model.dropout\n",
    "        }\n",
    "        self.mlflow_logger.log_hyperparams(hyperparams)\n",
    "        \n",
    "        # log criterion\n",
    "        self.criterion = criterion\n",
    "        self.mlflow_logger.log_hyperparams({'criterion': self.criterion.__name__})\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        X, y, seq_lengths = batch\n",
    "        logits = self.model(X, seq_lengths)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        X, y, seq_lengths = batch\n",
    "        logits = self.model(X, seq_lengths)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        X, y, seq_lengths = batch\n",
    "        logits = self.model(X, seq_lengths)\n",
    "        loss = self.criterion(logits, y)\n",
    "\n",
    "        # For metrics that require integer labels, convert one-hot encoded y to integers.\n",
    "        # Otherwise, provide probabilities using softmax\n",
    "        y_prob = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "        y_true_int = torch.argmax(y, dim=1).detach().cpu().numpy()\n",
    "        y_pred_int = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "\n",
    "        # Calculate the metrics\n",
    "        acc = accuracy_score(y_true_int, y_pred_int)\n",
    "        macro_f1 = f1_score(y_true_int, y_pred_int, average='macro', zero_division=0)\n",
    "        macro_precision = precision_score(y_true_int, y_pred_int, average='macro', zero_division=0)\n",
    "        macro_recall = recall_score(y_true_int, y_pred_int, average='macro', zero_division=0)\n",
    "        bal_acc = balanced_accuracy_score(y_true_int, y_pred_int)\n",
    "        macro_avg_precision = average_precision_score(y.detach().cpu().numpy(), y_prob, average='macro')\n",
    "        top_3_accuracy = top_k_accuracy_score(y_true_int, logits.detach().cpu(), k=3, labels=list(range(20)))\n",
    "\n",
    "        # Log the metrics\n",
    "        self.log('accuracy', acc)\n",
    "        self.log('macro_f1_score', macro_f1)\n",
    "        self.log('macro_precision_score', macro_precision)\n",
    "        self.log('macro_recall_score', macro_recall)\n",
    "        self.log('balanced_accuracy_score', bal_acc)\n",
    "        self.log('macro_average_precision_score', macro_avg_precision)\n",
    "        self.log('top_3_accuracy_score', top_3_accuracy)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # instantiate learning rate scheduler\n",
    "        self.mlflow_logger.log_hyperparams({\n",
    "            'warmup_steps': WARMUP_STEPS,\n",
    "            'cosine_steps': COSINE_STEPS,\n",
    "            'min_lr': MIN_LR,\n",
    "            'max_lr': MAX_LR\n",
    "        })\n",
    "\n",
    "        # instantiate optimizer\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=MAX_LR)\n",
    "        self.mlflow_logger.log_hyperparams({'optimizer': optimizer.__class__.__name__})\n",
    "\n",
    "        # Warmup: scales LR from 1e-3Ã—base to base LR over `warmup_steps`\n",
    "        warmup_scheduler = LinearLR(optimizer, start_factor=1e-4, end_factor=1.0, total_iters=WARMUP_STEPS)\n",
    "\n",
    "        # Cosine annealing: decays LR from base to Î·_min (1e-5) over `cosine_steps`\n",
    "        cosine_scheduler = CosineAnnealingLR(optimizer, T_max=COSINE_STEPS, eta_min=MIN_LR)\n",
    "\n",
    "        # Constant phase: hold the LR at eta_min.\n",
    "        constant_scheduler = ConstantLR(optimizer, factor=MIN_LR/MAX_LR, total_iters=1e10)\n",
    "\n",
    "        # Combine all three using SequentialLR.\n",
    "        scheduler = SequentialLR(\n",
    "            optimizer,\n",
    "            schedulers=[warmup_scheduler, cosine_scheduler, constant_scheduler],\n",
    "            milestones=[WARMUP_STEPS, WARMUP_STEPS + COSINE_STEPS]\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                      | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | model | RNN_Nationality_Predictor | 2.2 M  | train\n",
      "------------------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.798     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614fbc7f468e4a019707935a22986c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/haukesteffen/miniconda3/envs/LearningPyTorch/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301f933ea88645d3af92bd8744d75522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow_logger = MLFlowLogger(experiment_name='Nationality Predictor', log_model=True)\n",
    "\n",
    "# log training parameters\n",
    "params = {\n",
    "    \"max_epochs\": MAX_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "}\n",
    "mlflow_logger.log_hyperparams(params)\n",
    "\n",
    "model = RNN_Nationality_Predictor(\n",
    "    input_size=len(VOCABULARY)+1,\n",
    "    output_size=len(set(COUNTRY_MAPPING.values()))+1,\n",
    "    architecture='LSTM',\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_rnn_layers=NUM_RNN_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "lightning_model = LightningModelWrapper(\n",
    "    mlflow_logger=mlflow_logger,\n",
    "    model=model,\n",
    "    criterion=F.binary_cross_entropy_with_logits\n",
    ")\n",
    "\n",
    "# register callbacks\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "early_stopping = EarlyStopping('val_loss', patience=EARLY_STOPPING_PATIENCE) # patience counts checks, not steps, so changing val_check_interval in Trainer instantiation changes this behaviour\n",
    "callbacks = [lr_monitor, early_stopping]\n",
    "mlflow_logger.log_hyperparams({'callbacks': ', '.join([callback.__class__.__name__ for callback in callbacks])})\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    limit_val_batches=len(val_dataloader),\n",
    "    val_check_interval=N_EVAL,\n",
    "    log_every_n_steps=N_EVAL,\n",
    "    logger=mlflow_logger,\n",
    "    callbacks=callbacks,\n",
    "    gradient_clip_val=GRADIENT_CLIPPING_VAL\n",
    ")\n",
    "\n",
    "# fit model\n",
    "trainer.fit(\n",
    "    model=lightning_model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")\n",
    "\n",
    "# test model\n",
    "trainer.test(\n",
    "    test_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test names for all 19 regions\n",
    "test_names = {\n",
    "    # Africa\n",
    "    \"Northern Africa\": \"Abdel Fattah el-Sisi\", # Egypt\n",
    "    \"Middle Africa\": \"JoÃ£o LourenÃ§o\", # Angola\n",
    "    \"Western Africa\": \"Bola Ahmed Tinubu\", # Nigeria\n",
    "    \"Eastern Africa\": \"Taye Atske Selassie\", # Ethiopia\n",
    "    \"Southern Africa\": \"Cyril Ramaphosa\", # South Africa\n",
    "\n",
    "    # Asia\n",
    "    \"Central Asia\": \"Qassym-Schomart Kemeluly Toqajew\", # Kazakhstan\n",
    "    \"Eastern Asia\": \"Xi Jinping\", # China\n",
    "    \"South-Eastern Asia\": \"Prabowo Subianto\", # Indonesia\n",
    "    \"Southern Asia\": \"Droupadi Murmu\", # India\n",
    "    \"Western Asia\": \"Recep Tayyip ErdoÄŸan\", # Turkey\n",
    "\n",
    "    # Europe\n",
    "    \"Northern Europe\": \"Ulf Kristersson\", # Sweden\n",
    "    \"Western Europe\": \"Olaf Scholz\", # Germany\n",
    "    \"Southern Europe\": \"Giorgia Meloni\", # Italy\n",
    "    \"Eastern Europe\": \"Andrzej Sebastian Duda\", # Poland\n",
    "\n",
    "    # Americas\n",
    "    \"Northern America\": \"Donald Trump\", # United States\n",
    "    \"Central America\": \"AndrÃ©s Manuel LÃ³pez Obrador\", # Mexico\n",
    "    \"Caribbean\": \"Andrew Holness\", # Jamaica\n",
    "    \"South America\": \"Luiz InÃ¡cio Lula da Silva\", # Brazil\n",
    "\n",
    "    # Oceania\n",
    "    \"Oceania\": \"Anthony Albanese\" # Australia\n",
    "}\n",
    "\n",
    "# run test on test names\n",
    "lightning_model.model.eval()\n",
    "lightning_model.model.to(device)\n",
    "tensor, length = train_data._encode_name(list(test_names.values()))\n",
    "tensor = tensor.to(device)\n",
    "logits = lightning_model.model(tensor, length)\n",
    "countries_list = train_data._decode_country(logits)\n",
    "preds = dict(zip(test_names.values(), countries_list))\n",
    "\n",
    "# define column widths\n",
    "name_width = 40\n",
    "actual_width = 20\n",
    "predicted_width = 20\n",
    "correct_width = 10\n",
    "\n",
    "# print output header\n",
    "header = f\"{'Name':<{name_width}} {'Actual Class':<{actual_width}} {'Predicted Class':<{predicted_width}} {'Correct?':<{correct_width}}\"\n",
    "print(header)\n",
    "print(\"-\" * (name_width + actual_width + predicted_width + correct_width))\n",
    "\n",
    "# loop through test names and format outputs\n",
    "total = 0\n",
    "correct_count = 0\n",
    "for actual_class, name in test_names.items():\n",
    "    predicted_class = preds.get(name, \"N/A\")\n",
    "    is_correct = predicted_class == actual_class\n",
    "    correct_str = \"Yes\" if is_correct else \"No\"\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    total += 1\n",
    "    row = f\"{name:<{name_width}} {actual_class:<{actual_width}} {predicted_class:<{predicted_width}} {correct_str:<{correct_width}}\"\n",
    "    print(row)\n",
    "accuracy = (correct_count / total) * 100\n",
    "print(f'\\nAccuracy: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
