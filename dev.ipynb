{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NATIONALITY PREDICTION**\n",
    "\n",
    "The goal of this notebook is to create a model that can predict nationalities from name strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pycountry\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from unidecode import unidecode\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPORT DATA**\n",
    "\n",
    "- Import data from selected (todo: all) CSV files\n",
    "- Use n (todo: all) samples from imported dataframe\n",
    "- Concatenate dataframes, generate 'name' and 'nationality' columns\n",
    "- Create vocabularies for inputs and outputs:\n",
    "- Input vocabulary links characters to index integers and vice versa\n",
    "- Output vocabulary links nationalities to index integers and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 700_000\n",
    "country_codes = ['MX', 'RU', 'NO', 'IT', 'HK', 'AE', 'GB']\n",
    "#country_codes = [c.split('.')[0] for c in os.listdir('./data/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameNationalityData(Dataset):\n",
    "    \"\"\"Name Nationality dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, n_samples, country_codes):\n",
    "        self.padding_index = 0\n",
    "        countries = {}\n",
    "        for country in pycountry.countries:\n",
    "            countries[country.alpha_2] = country.name\n",
    "        dfs = []\n",
    "\n",
    "        for alpha2 in country_codes:\n",
    "            tmp = pd.read_csv(\n",
    "                './data/' + alpha2 + '.csv',\n",
    "                index_col=False,\n",
    "                header=None,\n",
    "                names=['forename', 'surname', 'gender', 'alpha2'],\n",
    "                dtype={'forename':'string', 'surname':'string', 'gender':'string', 'alpha2':'string'}\n",
    "            )\n",
    "            l = len(tmp)\n",
    "            tmp = tmp.dropna(subset=['forename', 'surname', 'alpha2'])\n",
    "            print(f'imported file: \"{alpha2}.csv\". number of records: {l}. dropped {l-len(tmp)} records because of missing values.')\n",
    "            tmp = tmp.sample(n=n_samples//len(country_codes))\n",
    "            dfs.append(tmp)\n",
    "\n",
    "        # concatenate country dataframes\n",
    "        df = pd.concat(dfs).reset_index(drop=True)\n",
    "        print(f'final dataset has {len(df)} records.')\n",
    "\n",
    "        # construct name and nationality, drop unnecessary columns\n",
    "        df['name'] = df['forename'] + ' ' + df['surname']\n",
    "        df = df[['name', 'alpha2']]\n",
    "\n",
    "        # generate input and output vocabularies\n",
    "        (self.ctoi_input,\n",
    "         self.itoc_input,\n",
    "         self.input_vocabulary_length) = self._generate_input_vocabulary(\n",
    "             df['name'].to_list()\n",
    "        )\n",
    "        (self.ctoi_output,\n",
    "         self.itoc_output,\n",
    "         self.output_vocabulary_length) = self._generate_output_vocabulary(\n",
    "             df['alpha2'].unique().to_list()\n",
    "        )\n",
    "\n",
    "        encoded_names = self._encode_input(df['name'].to_list()) # encode list of names as list of list of integers\n",
    "        self.sequence_lengths = torch.tensor([len(seq) for seq in encoded_names], dtype=torch.int32) # feature length vector for sequence padding in RNN\n",
    "        encoded_names = [torch.tensor(enc, dtype=torch.int32) for enc in encoded_names] # convert list of list of integers to list of tensors of integers\n",
    "        self.X = pad_sequence(encoded_names, batch_first=True) # use zero padding to convert tensors to equal lengths, outputs single tensor\n",
    "        encoded_countries = torch.tensor(self._encode_output(df['alpha2'].to_list()), dtype=torch.int64)\n",
    "        self.y = F.one_hot(encoded_countries, num_classes=self.output_vocabulary_length+1).to(torch.float32)\n",
    "\n",
    "\n",
    "    def _generate_input_vocabulary(self, names_list):\n",
    "        \"\"\"\n",
    "        Builds a vocabulary of unique characters from the provided list of names. \n",
    "        Each character is assigned an integer index, starting at 1 so that 0 can be \n",
    "        used as a padding index. This method also prints out the generated vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        names_list : list of str\n",
    "            A list of names from which the character vocabulary is constructed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of (dict, dict, int)\n",
    "            - ctoi_input : dict\n",
    "                Mapping from character to integer index.\n",
    "            - itoc_input : dict\n",
    "                Mapping from integer index back to character.\n",
    "            - input_vocabulary_length : int\n",
    "                The number of unique characters in input vocabulary\n",
    "        \"\"\"\n",
    "        input_vocabulary = sorted(list(set(''.join(names_list))))\n",
    "        input_vocabulary_length = len(input_vocabulary)\n",
    "        print(f\"Input vocabulary of length {len(input_vocabulary)}:\\n{''.join(input_vocabulary)}\")\n",
    "        ctoi_input = {c:i for i, c in enumerate(input_vocabulary, 1)} # start enumeration at 1 because 0 is padding index\n",
    "        itoc_input = {i:c for i, c in enumerate(input_vocabulary, 1)}\n",
    "        return ctoi_input, itoc_input, input_vocabulary_length\n",
    "\n",
    "    def _generate_output_vocabulary(self, alpha2_list):\n",
    "        \"\"\"\n",
    "        Builds a sorted list of unique output labels (country codes), then creates \n",
    "        mappings from label to integer index (starting at 1 so 0 can be used for padding) and from integer index \n",
    "        back to the label. It also returns the total number of unique labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha2_list : list of str\n",
    "            A list of country codes to be included in the output vocabulary.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of (dict, dict, int)\n",
    "            - ctoi_output : dict\n",
    "                Mapping from country code string to integer index.\n",
    "            - itoc_output : dict\n",
    "                Mapping from integer index back to the country code string.\n",
    "            - output_vocabulary_length : int\n",
    "                The number of unique output labels in the vocabulary.\n",
    "        \"\"\"\n",
    "        output_vocabulary = sorted(list(alpha2_list))\n",
    "        output_vocabulary_length = len(output_vocabulary)\n",
    "        print(f'Output vocabulary of length {len(output_vocabulary)}:\\n{output_vocabulary}')\n",
    "        ctoi_output = {c:i for i, c in enumerate(output_vocabulary, 1)} # start enumeration at 1 because 0 is padding index\n",
    "        itoc_output = {i:c for i, c in enumerate(output_vocabulary, 1)}\n",
    "        return ctoi_output, itoc_output, output_vocabulary_length\n",
    "\n",
    "    def _encode_input(self, seq):\n",
    "        \"\"\"\n",
    "        Encodes a string or a list of strings into a list (or list of lists) of integer indices \n",
    "        based on the `self.ctoi_input` dictionary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq : str or list of str\n",
    "            The input sequence(s) of characters to be converted into indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of int or list of list of int\n",
    "            A list of integer indices if `seq` is a single string, \n",
    "            or a list of lists of integer indices if `seq` is a list of strings.\n",
    "        \"\"\"\n",
    "        assert isinstance(seq, (str, list)), 'Input must be string or list of strings'\n",
    "        if isinstance(seq, str):\n",
    "            encoded_input = [self.ctoi_input.get(char, self.padding_index) for char in seq]\n",
    "        elif isinstance(seq, list):\n",
    "            encoded_input = []\n",
    "            for s in seq:\n",
    "                assert isinstance(s, str), 'Input must be string or list of strings'\n",
    "                encoded_input.append([self.ctoi_input.get(char, self.padding_index) for char in s])\n",
    "        return encoded_input\n",
    "    \n",
    "    def _decode_input(self, seq):\n",
    "        \"\"\"\n",
    "        Decodes a list of integer indices or a list of lists of integer indices into characters using the `self.itoc` mapping.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq : list of int or list of list of int\n",
    "            A list where each element can be an integer index (representing a character)\n",
    "            or a list of integer indices (representing multiple characters).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of str or list of list of str\n",
    "            A list of decoded characters (if individual elements are ints) or\n",
    "            a list of lists of decoded characters (if elements are lists of ints).\n",
    "        \"\"\"\n",
    "        assert isinstance(seq, list), 'Input must be list of integers or list of list of integers'\n",
    "        return [self.itoc_input.get(s, '') if isinstance(s, int) else [self.itoc_input.get(index, '') for index in s] for s in seq]\n",
    "\n",
    "    def _encode_output(self, country_code):\n",
    "        \"\"\"\n",
    "        Encodes a single country code string or a list of country code strings into their \n",
    "        corresponding integer index using `self.ctoi_output`. If a code is not found in the mapping,\n",
    "        `self.padding_index` is used as a fallback.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        country_code : str or list of str\n",
    "            The country code(s) to encode.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int or list of int\n",
    "            An integer index if `country_code` is a single string,\n",
    "            or a list of integer indices if `country_code` is a list.\n",
    "        \"\"\"\n",
    "        assert isinstance(country_code, (str, list)), 'Input must be string or list of strings'\n",
    "        if isinstance(country_code, str):\n",
    "            encoded_output = self.ctoi_output.get(country_code, self.padding_index)\n",
    "        elif isinstance(country_code, list):\n",
    "            encoded_output = []\n",
    "            for c in country_code:\n",
    "                assert isinstance(c, str), 'Input must be string or list of strings'\n",
    "                encoded_output.append(self.ctoi_output.get(c, self.padding_index))\n",
    "        return encoded_output\n",
    "    \n",
    "    def _decode_output(self, country_code):\n",
    "        \"\"\"\n",
    "        Decodes a single integer or a list of integer country codes into their corresponding\n",
    "        string labels using the `self.itoc` dictionary. If a code is not found in the mapping,\n",
    "        'Unknown' is returned.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        country_code : int or list of int\n",
    "            The country code(s) to decode.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str or list of str\n",
    "            A single decoded string if `country_code` is an integer,\n",
    "            or a list of decoded strings if `country_code` is a list.\n",
    "        \"\"\"\n",
    "        assert isinstance(country_code, (int, list)), 'Input must be integer or list of integers'\n",
    "        if isinstance(country_code, int):\n",
    "            return self.itoc_output.get(country_code, 'Unknown')\n",
    "        elif isinstance(country_code, list):\n",
    "            decoded_output = []\n",
    "            for index in country_code:\n",
    "                assert isinstance(index, int), 'Input must be integer or list of integers'\n",
    "                decoded_output.append(self.itoc_output.get(index, 'Unknown'))\n",
    "            return decoded_output\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        sequence_length = self.sequence_lengths[idx]\n",
    "        return (X, y, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported file: \"MX.csv\". number of records: 13330219. dropped 43260 records because of missing values.\n",
      "imported file: \"RU.csv\". number of records: 9992686. dropped 17069 records because of missing values.\n",
      "imported file: \"NO.csv\". number of records: 475782. dropped 1850 records because of missing values.\n",
      "imported file: \"IT.csv\". number of records: 35554357. dropped 69126 records because of missing values.\n",
      "imported file: \"HK.csv\". number of records: 2846829. dropped 293650 records because of missing values.\n",
      "imported file: \"AE.csv\". number of records: 6792773. dropped 54115 records because of missing values.\n",
      "imported file: \"GB.csv\". number of records: 11519228. dropped 24981 records because of missing values.\n",
      "final dataset has 700000 records.\n",
      "Input vocabulary of length 1909:\n",
      " &-.ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyzÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÚÛÜÞßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýþÿĀāĂăĄąĆćČčĎďĐđĒēĔĕėęĚěĜĝĞğĢģĥħĩĪīĭĮįİıĴĶķĸĹĺļĽľŁłńņŇňŋōŐőœŔŕŗŘřŚśŜŝŞşŠšŢţŤťŨũŪūůűųŵŷŸŹźŻżŽžƁƊƎƏơƤƭưƱƶǎǝǤǥǮǻǿȃȘșȚțȝɀɓɘəɞɩɪɭɱɾʀʋʌʚʛʟ̶̲̅ΊΐΑΓΔΕΖΗΘΙΚΜΞΟΠΣΦΧΫάέήίΰαβγδεηθικλμνξοπρςστυφχψωϊόώϕЁЄІАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяёђѓєіїјљћў҉ғүҲҳҷӣөԱԳԹԿՀՂՄՌՍՏՔՕագեզէթիկհղմյնոպստրւևאבגדוזחטילמןסעפצר،ءآأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىيٰٓٛٱٹټپځچڈڍڑړڕژکګگڵںڼھہیۍېے۔ەۣۗۙۧँंअआइईउएकखगघङचछजटठडढणतथदधनपफबभमयरलवशषसह़ािीुूृेैॉोौ्ॐঁংঃঅআইঈউএওকখগঘঙচছজঝটঠডঢণতথদধনপফবভমযরলশষসহ়ািীুূৃেৈোৌ্ৎਂਅਆਇਊਓਕਖਗਘਚਛਜਟਡਣਤਦਧਨਪਫਬਭਮਯਰਲਵਸਹ਼ਾਿੀੁੇੈੋੌੰੱઈખગજટડતનમયરલસાીુૅેଂକଜନମରାୁஅஆஇஈஉஎஒகஙசஜடணதநனபமயரறலளழவஷஸஹாிீுூெேைொோ்ఊకచజడతదనపమరలవహాిీుూే్ಌംഅആഇഉഎഏകഗങചജഞടണതദനപഫബമയരറലളഴവശഷസഹാിീുൂൃെേൈൊോ്ൗൻർൽൾංඅඇඉකගඟචජඩණතදනපමයරලවශෂසහ්ාිීුේกขคฅงจฉชซญฐฑฒณดตถทธนบปผฝพฟภมยรฤลวศษสหอฮะัาำิีึืุูเแโใไ็่้๊๋์་ཀགཆདནཔབམརལིོླကဂငစဆတထပမယရလဝသအဦါာိီုူေဲဳး္်ျြၾႀႈႊაბგდევიკლმნოპრსტუქყჩძხჯሀሁሂሃሄህሆለሉሊላሌልሎሐሓሔሕመሙሚማሜምሞሠሢሥረሩሪራሬርሮሰሱሲሳሴስሶሸሻሽቀቁቂቃቄቅቆቋቕበቡቢባቤብቦተቱቲታቴቷቸቹቺቻችነኑኒናኔንኖኛኝአኡኢኣኤእኦከኩኪካኬክኮኳኹኽኾወዉዋዌውዎዑዓዕዘዙዚዛዜዝየዩዪያዬይዮደዱዲዳዴድዶጀጂጃጅገጉጊጋጌግጎጓጠጡጣጤጥጦጨጪጫጸጺጽፀፂፅፈፊፋፍ፡ᐸᒶᓋᓌᓧᓿᔙᔜᕬᕱᕿᖆᖺᖻᖽᗁᗅᗰᗿᘢᘴᙏᜁᜄᜆᜇᜎᜒᜓ᜔កមយសួៀḕṁṧẞạảấầẩẫậằẳẶặếềểễệỉịọỏỐốồổỗộớờởủứữỳỵỷỹ✿ぁあいうえおかきぎくこしたちつてとなにのひふほまみめもやゅゆょよりるれろをんァアィイゥウエォオカガキギクコゴサザシジスズセタダチヂッツヅテデトドナニネノハヒビピフブプヘベホポマミムメモャヤュユョラリルレワンヴ・ー㋛一丁三上不世並中丸丹主久之九亂事二于五井亚亦人亼仁仈今仔代仪休伯但佑佬佳使來依係保倫假偉健傑備傲傳儿元兆先克兒兔公六再冧冰冷凌几凱切初利前剎剛劉力助動務勝北匚十千华博原友叔叮可司吉同名向君吳吹呀呂咀哥唯啤善喜單嘉噹嚮嚼四國園土在地坊坤型埃城基堅場塵士声壹夏多夢大天太失奈奥女奶好如妍妹姐婦子字學宇安宏宗官宙定宜実室宮家容富寵寶將專對小少尘尤尺展屬山岑岡岩岸島崇崎川工巧巨布希常平幸庭廣廿弦張強彥彦影彼待徐徒得御德心忌志怡恵情意愛慈慮慶憂應懶戀拉掛撈攻效敬文斗斯新方旅旗日旧昌明星時晉晞景晴智曼月有服期木本李杏村杰東松枒林果柔栄栗栢格桃梁椒椰楊楓楠業極榮樂樹樺機檸次欣欧歐武残每比毛氏水永汁江汶治波泰洋洗洛浩浪海涛淘淚深淳渕港湛滙漏漢潔澤瀬火炎炒無焯然煒熙熱燈燕爆爐爱父牙物猫玉王玟玥玦玲玻珍珠球理琉琪璃瓦生產田由男番痕痛瘦皇益直眉眠矮磚社神祤祥福秀私秋科程穗究竣竹篠籬米粵糕紗級細維綾緊纖缘置罰羅羊美羽翹耀聯聰聲育胤腦臨自興舍舖舞船良艷艾芦花苦英茂茅茶莊華落葉葫蔥蕊藤蘇號蛇蜚蜜螺裂裕襲西要規角記詩談謙講讀谷象豪貓賢超車軍軒転輝辉辛辣迪進逸遊過道達郎部郭都鄧酒醉里野金鈴鈺錦鍾鏡鑽長門開阳阿限陳陽隊際雄雅雞雪雲電霆靈靚面音韻頭風飄飽香馬馮高髮鬼魚鯊鴻鹅鹿麟麥麵黃黒點龍나라래러루림마부빈사수시야얼오이지허ﮔﮩﯾﯿﹻﺃﺍﺎﺏﺑﺒﺕﺟﺣﺤﺧﺪﺫﺭﺮﺯﺳﺴﺻﺼﺿﻀﻃﻄﻋﻌﻒﻓﻔﻘﻛﻜﻝﻞﻟﻠﻡﻣﻤﻥﻦﻧﻨﻪﻫﻬﻮﻯﻰﻱﻲﻳﻴﻷﻻ\n",
      "Output vocabulary of length 7:\n",
      "['AE', 'GB', 'HK', 'IT', 'MX', 'NO', 'RU']\n"
     ]
    }
   ],
   "source": [
    "dataset = NameNationalityData(n_samples=n_samples, country_codes=country_codes)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_samples*4//5, n_samples*1//5])\n",
    "train_dataloader = DataLoader(train_set, batch_size=1024, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=len(val_set), shuffle=True, drop_last=True)\n",
    "\n",
    "input_vocabulary_length = dataset.input_vocabulary_length\n",
    "output_vocabulary_length = dataset.output_vocabulary_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MODELING**\n",
    "\n",
    "- Create simple model using character embeddings, rnn layers and a dense layer (todo: dropout, weight initialization)\n",
    "- Find best initial learning rate\n",
    "- Get a baseline crossentropy loss\n",
    "- Get a model to overfit sample data\n",
    "- train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Nationality_Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=dataset.input_vocabulary_length+1,\n",
    "            embedding_dim=32,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=32,\n",
    "            hidden_size=64,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=64,\n",
    "            out_features=dataset.output_vocabulary_length+1,\n",
    "        )\n",
    "\n",
    "    def forward(self, X, lengths):\n",
    "        embeddings = self.embed(X)\n",
    "\n",
    "        # Pack the padded batch\n",
    "        packed = pack_padded_sequence(\n",
    "            embeddings,\n",
    "            lengths=lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        _, hidden = self.rnn(packed)\n",
    "        logits = self.dense(hidden[-1])\n",
    "        output = F.softmax(logits, dim=0)\n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_Nationality_Predictor().to(device)\n",
    "criterion = F.binary_cross_entropy_with_logits\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- EPOCH 1 -------------------------\n",
      "mean training loss over last 100 batches: 0.69696 --- validation loss: 0.68780\n",
      "mean training loss over last 100 batches: 0.43860 --- validation loss: 0.36159\n",
      "mean training loss over last 100 batches: 0.34971 --- validation loss: 0.33065\n",
      "mean training loss over last 100 batches: 0.31528 --- validation loss: 0.29598\n",
      "mean training loss over last 100 batches: 0.28495 --- validation loss: 0.26570\n",
      "mean training loss over last 100 batches: 0.25834 --- validation loss: 0.24234\n",
      "------------------------- EPOCH 2 -------------------------\n",
      "mean training loss over last 100 batches: 0.24852 --- validation loss: 0.23426\n",
      "mean training loss over last 100 batches: 0.23344 --- validation loss: 0.22133\n",
      "mean training loss over last 100 batches: 0.22166 --- validation loss: 0.21089\n",
      "mean training loss over last 100 batches: 0.21147 --- validation loss: 0.20166\n",
      "mean training loss over last 100 batches: 0.20350 --- validation loss: 0.19482\n",
      "mean training loss over last 100 batches: 0.19786 --- validation loss: 0.18817\n",
      "------------------------- EPOCH 3 -------------------------\n",
      "mean training loss over last 100 batches: 0.19573 --- validation loss: 0.18604\n",
      "mean training loss over last 100 batches: 0.18998 --- validation loss: 0.18185\n",
      "mean training loss over last 100 batches: 0.18672 --- validation loss: 0.17884\n",
      "mean training loss over last 100 batches: 0.18449 --- validation loss: 0.17511\n",
      "mean training loss over last 100 batches: 0.17948 --- validation loss: 0.17151\n",
      "mean training loss over last 100 batches: 0.17675 --- validation loss: 0.16889\n",
      "------------------------- EPOCH 4 -------------------------\n",
      "mean training loss over last 100 batches: 0.17500 --- validation loss: 0.16811\n",
      "mean training loss over last 100 batches: 0.17374 --- validation loss: 0.16539\n",
      "mean training loss over last 100 batches: 0.17146 --- validation loss: 0.16371\n",
      "mean training loss over last 100 batches: 0.16949 --- validation loss: 0.16186\n",
      "mean training loss over last 100 batches: 0.16901 --- validation loss: 0.16040\n",
      "mean training loss over last 100 batches: 0.16636 --- validation loss: 0.15914\n",
      "------------------------- EPOCH 5 -------------------------\n",
      "mean training loss over last 100 batches: 0.16539 --- validation loss: 0.15940\n",
      "mean training loss over last 100 batches: 0.16498 --- validation loss: 0.15939\n",
      "mean training loss over last 100 batches: 0.16401 --- validation loss: 0.15651\n",
      "mean training loss over last 100 batches: 0.16339 --- validation loss: 0.15546\n",
      "mean training loss over last 100 batches: 0.16125 --- validation loss: 0.15455\n",
      "mean training loss over last 100 batches: 0.16148 --- validation loss: 0.15422\n",
      "------------------------- EPOCH 6 -------------------------\n",
      "mean training loss over last 100 batches: 0.16144 --- validation loss: 0.15337\n",
      "mean training loss over last 100 batches: 0.15951 --- validation loss: 0.15264\n",
      "mean training loss over last 100 batches: 0.15912 --- validation loss: 0.15150\n",
      "mean training loss over last 100 batches: 0.15865 --- validation loss: 0.15257\n",
      "mean training loss over last 100 batches: 0.15810 --- validation loss: 0.15110\n",
      "mean training loss over last 100 batches: 0.15709 --- validation loss: 0.15037\n",
      "------------------------- EPOCH 7 -------------------------\n",
      "mean training loss over last 100 batches: 0.15679 --- validation loss: 0.14980\n",
      "mean training loss over last 100 batches: 0.15480 --- validation loss: 0.14951\n",
      "mean training loss over last 100 batches: 0.15703 --- validation loss: 0.14846\n",
      "mean training loss over last 100 batches: 0.15533 --- validation loss: 0.14788\n",
      "mean training loss over last 100 batches: 0.15500 --- validation loss: 0.14813\n",
      "mean training loss over last 100 batches: 0.15315 --- validation loss: 0.14670\n",
      "------------------------- EPOCH 8 -------------------------\n",
      "mean training loss over last 100 batches: 0.15347 --- validation loss: 0.14680\n",
      "mean training loss over last 100 batches: 0.15310 --- validation loss: 0.14651\n",
      "mean training loss over last 100 batches: 0.15251 --- validation loss: 0.14620\n",
      "mean training loss over last 100 batches: 0.15162 --- validation loss: 0.14538\n",
      "mean training loss over last 100 batches: 0.15279 --- validation loss: 0.14484\n",
      "mean training loss over last 100 batches: 0.15240 --- validation loss: 0.14453\n",
      "------------------------- EPOCH 9 -------------------------\n",
      "mean training loss over last 100 batches: 0.15135 --- validation loss: 0.14479\n",
      "mean training loss over last 100 batches: 0.15080 --- validation loss: 0.14431\n",
      "mean training loss over last 100 batches: 0.15009 --- validation loss: 0.14503\n",
      "mean training loss over last 100 batches: 0.15079 --- validation loss: 0.14418\n",
      "mean training loss over last 100 batches: 0.14927 --- validation loss: 0.14333\n",
      "mean training loss over last 100 batches: 0.14974 --- validation loss: 0.14229\n",
      "------------------------- EPOCH 10 -------------------------\n",
      "mean training loss over last 100 batches: 0.14918 --- validation loss: 0.14315\n",
      "mean training loss over last 100 batches: 0.14971 --- validation loss: 0.14173\n",
      "mean training loss over last 100 batches: 0.14865 --- validation loss: 0.14227\n",
      "mean training loss over last 100 batches: 0.14877 --- validation loss: 0.14184\n",
      "mean training loss over last 100 batches: 0.14623 --- validation loss: 0.14136\n",
      "mean training loss over last 100 batches: 0.14643 --- validation loss: 0.14122\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "eval_every_n_batches = 100\n",
    "for epoch in range(10):\n",
    "    print(f'-'*25+f' EPOCH {epoch+1} '+f'-'*25)\n",
    "    for batch, (X, y, sequence_lenghts) in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits, _ = model(X, sequence_lenghts)\n",
    "        loss = criterion(logits, y)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch%eval_every_n_batches==0:\n",
    "            model.eval()\n",
    "            X, y, sequence_lenghts = next(iter(val_dataloader))\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits, _ = model(X, sequence_lenghts)\n",
    "            val_loss = criterion(logits, y)\n",
    "            print(f'mean training loss over last {eval_every_n_batches} batches: {np.mean(losses[-eval_every_n_batches:]):.5f} --- validation loss: {val_loss:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
