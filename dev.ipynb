{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NATIONALITY PREDICTION**\n",
    "\n",
    "The goal of this notebook is to create a model that can predict nationalities from name strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pycountry\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from unidecode import unidecode\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPORT DATA**\n",
    "\n",
    "- Import data from selected (todo: all) CSV files\n",
    "- Use n (todo: all) samples from imported dataframe\n",
    "- Concatenate dataframes, generate 'name' and 'nationality' columns\n",
    "- Create vocabularies for inputs and outputs:\n",
    "- Input vocabulary links characters to index integers and vice versa\n",
    "- Output vocabulary links nationalities to index integers and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 70_000\n",
    "country_codes = ['MX', 'RU', 'NO', 'IT', 'HK', 'AE', 'GB']\n",
    "#country_codes = [c.split('.')[0] for c in os.listdir('./data/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameNationalityData(Dataset):\n",
    "    \"\"\"Name Nationality dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, n_samples, country_codes):\n",
    "        self.padding_index = 0\n",
    "        countries = {}\n",
    "        for country in pycountry.countries:\n",
    "            countries[country.alpha_2] = country.name\n",
    "        dfs = []\n",
    "\n",
    "        for alpha2 in country_codes:\n",
    "            tmp = pd.read_csv(\n",
    "                './data/' + alpha2 + '.csv',\n",
    "                index_col=False,\n",
    "                header=None,\n",
    "                names=['forename', 'surname', 'gender', 'alpha2'],\n",
    "                dtype={'forename':'string', 'surname':'string', 'gender':'string', 'alpha2':'string'}\n",
    "            )\n",
    "            l = len(tmp)\n",
    "            tmp = tmp.dropna(subset=['forename', 'surname', 'alpha2'])\n",
    "            print(f'imported file: \"{alpha2}.csv\". number of records: {l}. dropped {l-len(tmp)} records because of missing values.')\n",
    "            tmp = tmp.sample(n=n_samples//len(country_codes))\n",
    "            dfs.append(tmp)\n",
    "\n",
    "        # concatenate country dataframes\n",
    "        df = pd.concat(dfs).reset_index(drop=True)\n",
    "        print(f'final dataset has {len(df)} records.')\n",
    "\n",
    "        # construct name and nationality, drop unnecessary columns\n",
    "        df['name'] = df['forename'] + ' ' + df['surname']\n",
    "        df = df[['name', 'alpha2']]\n",
    "        self.maximum_name_length = df['name'].str.len().max()\n",
    "\n",
    "        # generate input and output vocabularies\n",
    "        (self.ctoi_input,\n",
    "         self.itoc_input,\n",
    "         self.input_vocabulary_length) = self._generate_input_vocabulary(\n",
    "             df['name'].to_list()\n",
    "        )\n",
    "        (self.ctoi_output,\n",
    "         self.itoc_output,\n",
    "         self.output_vocabulary_length) = self._generate_output_vocabulary(\n",
    "             df['alpha2'].unique()\n",
    "        )\n",
    "\n",
    "        # encode inputs as padded index tensors\n",
    "        (self.X,\n",
    "         self.sequence_lengths) = self._encode_input(\n",
    "             df['name'].to_list()\n",
    "        )\n",
    "        # encode outputs as one-hot index\n",
    "        self.y = self._encode_output(\n",
    "            df['alpha2'].to_list()\n",
    "        )\n",
    "\n",
    "\n",
    "    def _generate_input_vocabulary(self, names_list):\n",
    "        \"\"\"\n",
    "        Builds a vocabulary of unique characters from the provided list of names. \n",
    "        Each character is assigned an integer index, starting at 1 so that 0 can be \n",
    "        used as a padding index. This method also prints out the generated vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        names_list : list of str\n",
    "            A list of names from which the character vocabulary is constructed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of (dict, dict, int)\n",
    "            - ctoi_input : dict\n",
    "                Mapping from character to integer index.\n",
    "            - itoc_input : dict\n",
    "                Mapping from integer index back to character.\n",
    "            - input_vocabulary_length : int\n",
    "                The number of unique characters in input vocabulary\n",
    "        \"\"\"\n",
    "        input_vocabulary = sorted(list(set(''.join(names_list))))\n",
    "        input_vocabulary_length = len(input_vocabulary)\n",
    "        print(f\"Input vocabulary of length {len(input_vocabulary)}:\\n{''.join(input_vocabulary)}\")\n",
    "        ctoi_input = {c:i for i, c in enumerate(input_vocabulary, 1)} # start enumeration at 1 because 0 is padding index\n",
    "        itoc_input = {i:c for i, c in enumerate(input_vocabulary, 1)}\n",
    "        return ctoi_input, itoc_input, input_vocabulary_length\n",
    "\n",
    "    def _generate_output_vocabulary(self, alpha2_list):\n",
    "        \"\"\"\n",
    "        Builds a sorted list of unique output labels (country codes), then creates \n",
    "        mappings from label to integer index (starting at 1 so 0 can be used for padding) and from integer index \n",
    "        back to the label. It also returns the total number of unique labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha2_list : list of str\n",
    "            A list of country codes to be included in the output vocabulary.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of (dict, dict, int)\n",
    "            - ctoi_output : dict\n",
    "                Mapping from country code string to integer index.\n",
    "            - itoc_output : dict\n",
    "                Mapping from integer index back to the country code string.\n",
    "            - output_vocabulary_length : int\n",
    "                The number of unique output labels in the vocabulary.\n",
    "        \"\"\"\n",
    "        output_vocabulary = sorted(list(alpha2_list))\n",
    "        output_vocabulary_length = len(output_vocabulary)\n",
    "        print(f'Output vocabulary of length {len(output_vocabulary)}:\\n{output_vocabulary}')\n",
    "        ctoi_output = {c:i for i, c in enumerate(output_vocabulary, 1)} # start enumeration at 1 because 0 is padding index\n",
    "        itoc_output = {i:c for i, c in enumerate(output_vocabulary, 1)}\n",
    "        return ctoi_output, itoc_output, output_vocabulary_length\n",
    "\n",
    "    def _encode_input(self, seq):\n",
    "        \"\"\"\n",
    "        Encodes a single string or a list of strings into integer indices based on `self.ctoi_input`,\n",
    "        replacing unmapped characters with `self.padding_index`. Each encoded sequence is then padded\n",
    "        to `self.maximum_name_length`, and the original (unpadded) lengths are recorded.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq : str or list of str\n",
    "            The input string(s) to be converted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (torch.Tensor, torch.Tensor)\n",
    "            - padded_tensors : torch.Tensor\n",
    "                A batch-first tensor of shape (batch_size, self.maximum_name_length) containing \n",
    "                the padded integer-encoded sequences.\n",
    "            - sequence_lengths : torch.Tensor\n",
    "                A tensor of shape (batch_size,) indicating the original lengths of each sequence \n",
    "                before padding.\n",
    "        \"\"\"\n",
    "        assert isinstance(seq, (str, list)), \"Input must be string or list of strings\"\n",
    "        if isinstance(seq, str): \n",
    "            seq = [seq] # wrap single string into a list of a single string\n",
    "        encoded_input = []\n",
    "        for s in seq:\n",
    "            assert isinstance(s, str), \"Each element in the list must be a string\"\n",
    "            # Convert each character to an index, defaulting to padding_index if not found\n",
    "            encoded_input.append([self.ctoi_input.get(char, self.padding_index) for char in s])\n",
    "        sequence_lengths = torch.tensor([len(encoding) for encoding in encoded_input], dtype=torch.int32)\n",
    "        # create empty tensor\n",
    "        batch_size = len(encoded_input)\n",
    "        padded_tensors = torch.full(\n",
    "            (batch_size, self.maximum_name_length),\n",
    "            self.padding_index,\n",
    "            dtype=torch.int32\n",
    "        )\n",
    "        # fill empty tensor with actual data\n",
    "        for i, encoding in enumerate(encoded_input):\n",
    "            seq_len = len(encoding)\n",
    "            # truncate if the sequence is longer than maximum_name_length\n",
    "            max_len = min(seq_len, self.maximum_name_length)\n",
    "            padded_tensors[i, :max_len] = torch.tensor(encoding[:max_len], dtype=torch.int32)\n",
    "        return padded_tensors, sequence_lengths\n",
    " \n",
    "    def _decode_input(self, seq_tensor):\n",
    "        \"\"\"\n",
    "        Decodes a 1D or 2D tensor of integer indices into characters using the `self.itoc_input` mapping.\n",
    "        \n",
    "        - If `seq_tensor` is 1D (shape: [N]), it decodes a single sequence of characters.\n",
    "        - If `seq_tensor` is 2D (shape: [B, N]), it decodes multiple sequences (one per row).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_tensor : torch.Tensor\n",
    "            A 1D or 2D tensor of integer indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of str or list of list of str\n",
    "            - If `seq_tensor` is 1D, returns a list of decoded characters for that single sequence.\n",
    "            - If `seq_tensor` is 2D, returns a list of lists of decoded characters, one list per row.\n",
    "        \"\"\"\n",
    "        if not isinstance(seq_tensor, torch.Tensor):\n",
    "            raise TypeError(\"seq_tensor must be a torch.Tensor of integer indices.\")\n",
    "        if seq_tensor.dim() == 1:\n",
    "            return ''.join([self.itoc_input.get(int(idx), '') for idx in seq_tensor])\n",
    "        elif seq_tensor.dim() == 2:\n",
    "            decoded_sequences = []\n",
    "            for row in seq_tensor:\n",
    "                decoded_sequences.append(''.join([self.itoc_input.get(int(idx), '') for idx in row]))\n",
    "            return decoded_sequences\n",
    "        else:\n",
    "            raise ValueError(\"seq_tensor must be a 1D or 2D tensor of integer indices.\")\n",
    "\n",
    "    def _encode_output(self, country_code):\n",
    "        \"\"\"\n",
    "        Encodes a single country code string or a list of strings into integer indices \n",
    "        using `self.ctoi_output`, then converts them to a one-hot representation. Any \n",
    "        code not found in `self.ctoi_output` is mapped to `self.padding_index`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        country_code : str or list of str\n",
    "            The country code(s) to encode.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A one-hot encoded tensor (dtype=torch.float64) of shape \n",
    "            (num_items, self.output_vocabulary_length + 1), \n",
    "            where 'num_items' is 1 if `country_code` is a single string, \n",
    "            or len(country_code) if it is a list.\n",
    "        \"\"\"\n",
    "        assert isinstance(country_code, (str, list)), 'Input must be string or list of strings'\n",
    "        if isinstance(country_code, str):\n",
    "            encoded_output = [self.ctoi_output.get(country_code, self.padding_index)]\n",
    "        elif isinstance(country_code, list):\n",
    "            encoded_output = []\n",
    "            for c in country_code:\n",
    "                assert isinstance(c, str), 'Input must be string or list of strings'\n",
    "                encoded_output.append(self.ctoi_output.get(c, self.padding_index))\n",
    "        index_tensors = torch.tensor(encoded_output, dtype=torch.int64)\n",
    "        encoded_tensors = F.one_hot(index_tensors, num_classes=self.output_vocabulary_length+1).to(torch.float64)\n",
    "        return encoded_tensors      \n",
    "\n",
    "    def _decode_output(self, country_code_tensor):\n",
    "        \"\"\"\n",
    "        Decodes a single integer or a list of integer country codes into their corresponding\n",
    "        string labels using the `self.itoc` dictionary. If a code is not found in the mapping,\n",
    "        'Unknown' is returned.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        country_code : int or list of int\n",
    "            The country code(s) to decode.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str or list of str\n",
    "            A single decoded string if `country_code` is an integer,\n",
    "            or a list of decoded strings if `country_code` is a list.\n",
    "        \"\"\"\n",
    "        if not isinstance(country_code_tensor, torch.Tensor):\n",
    "            raise TypeError(\"country_code_tensor must be a torch.Tensor of integer indices.\")\n",
    "        if country_code_tensor.dim() == 1:\n",
    "            return [self.itoc_output.get(torch.argmax(country_code_tensor).item(), 'Unknown')]\n",
    "        elif country_code_tensor.dim() == 2:\n",
    "            decoded_output = []\n",
    "            for encoding in country_code_tensor:\n",
    "                index = torch.argmax(encoding).item()\n",
    "                decoded_output.append(self.itoc_output.get(index, 'Unknown'))\n",
    "            return decoded_output\n",
    "        else:\n",
    "            raise ValueError(\"country_code_tensor must be a 1D or 2D tensor of integer indices.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        sequence_length = self.sequence_lengths[idx]\n",
    "        return (X, y, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported file: \"MX.csv\". number of records: 13330219. dropped 43260 records because of missing values.\n",
      "imported file: \"RU.csv\". number of records: 9992686. dropped 17069 records because of missing values.\n",
      "imported file: \"NO.csv\". number of records: 475782. dropped 1850 records because of missing values.\n",
      "imported file: \"IT.csv\". number of records: 35554357. dropped 69126 records because of missing values.\n",
      "imported file: \"HK.csv\". number of records: 2846829. dropped 293650 records because of missing values.\n",
      "imported file: \"AE.csv\". number of records: 6792773. dropped 54115 records because of missing values.\n",
      "imported file: \"GB.csv\". number of records: 11519228. dropped 24981 records because of missing values.\n",
      "final dataset has 70000 records.\n",
      "Input vocabulary of length 730:\n",
      " -.ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÀÁÂÃÄÅÈÉÍÎÏÑÓÖØÜÞàáâãäåæçèéêëìíîïðñòóôõöøùúûüýþāăąĆćčďĐđēėęĞğĥĨĩīįİıĶĹĺļľŁłńņňőŕřŚśŞşŠšŤťũŪūųŸŹźżŽžǎȘțɞʚʬ̃ΒΕΜΝαειλνοπστυχύЄІАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЫЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяёіүӨՄՍադեըլհմյնովւאבגהזחילערءآأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىيٰٹپچکګگڵھہیے۔ंअऊकखगचजठडतदधनफबमयरलवशषसह़ािीुूेैोौ्ংঃঅআইউঊএকখগঙচছজঞটণতথদনপফবভমযরলশষসহ়ািীুেৈো্ৎਂਗਘਜਤਦਯਰਸਾਿੋੰஅகசஜணதனபமயரலழவஷஹாிுெேை்ംഎഖദപഫറീു്දනරශ්ාිกขคงจฉชญฐฒณดตทนบปพภมยรลวศษสหอะัาำิีืุูเแโใไ็่้์ခစတမသအိူ်ျაევთიკლმნუშሀሁሃህሆለሉሊላሌልሎመሚማምሣረሩሪራሬርሰሱሳስቀቁቃቅበባቤብተቱቲታቴችነኑናኔንኖኛኝአኤእከኩኪካኮወዉዊዋውዑዕዘዝዤየዩያይደዱዴድዶጂጅገጋግጠጡጣጭፁፈፊፋፍḍṛạầậắằềễệỏồộủ⑩✯いうえきくけしだにのほぼツ一二人仁仔伍伯伸何偉健八利北千友司周嘉地基堡壹大奇子家寇寶小就希弋強彭徒得情慧戀手拜敏敬明林梦業歐殺毛治海牆瑞由男白真神立紅純肆舞芊茵萬虎號詩足追醉陸陽隻雅電香黃구나반웃ﺍﺑﺒﺣﻪﻮﻴ\n",
      "Output vocabulary of length 7:\n",
      "['AE', 'GB', 'HK', 'IT', 'MX', 'NO', 'RU']\n"
     ]
    }
   ],
   "source": [
    "dataset = NameNationalityData(n_samples=n_samples, country_codes=country_codes)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "train_dataloader = DataLoader(train_set, batch_size=1024, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=len(val_set), shuffle=True, drop_last=True)\n",
    "\n",
    "input_vocabulary_length = dataset.input_vocabulary_length\n",
    "output_vocabulary_length = dataset.output_vocabulary_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MODELING**\n",
    "\n",
    "- Create simple model using character embeddings, rnn layers and a dense layer (todo: dropout, weight initialization)\n",
    "- Find best initial learning rate\n",
    "- Get a baseline crossentropy loss\n",
    "- Get a model to overfit sample data\n",
    "- train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Nationality_Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=dataset.input_vocabulary_length+1,\n",
    "            embedding_dim=32,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=32,\n",
    "            hidden_size=64,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=64,\n",
    "            out_features=dataset.output_vocabulary_length+1,\n",
    "        )\n",
    "\n",
    "    def forward(self, X, lengths):\n",
    "        embeddings = self.embed(X)\n",
    "\n",
    "        # Pack the padded batch\n",
    "        packed = pack_padded_sequence(\n",
    "            embeddings,\n",
    "            lengths=lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        _, hidden = self.rnn(packed)\n",
    "        logits = self.dense(hidden[-1])\n",
    "        output = F.softmax(logits, dim=0)\n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_Nationality_Predictor().to(device)\n",
    "criterion = F.binary_cross_entropy_with_logits\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- EPOCH 1 -------------------------\n",
      "mean training loss over last 100 batches: 0.69696 --- validation loss: 0.68780\n",
      "mean training loss over last 100 batches: 0.43860 --- validation loss: 0.36159\n",
      "mean training loss over last 100 batches: 0.34971 --- validation loss: 0.33065\n",
      "mean training loss over last 100 batches: 0.31528 --- validation loss: 0.29598\n",
      "mean training loss over last 100 batches: 0.28495 --- validation loss: 0.26570\n",
      "mean training loss over last 100 batches: 0.25834 --- validation loss: 0.24234\n",
      "------------------------- EPOCH 2 -------------------------\n",
      "mean training loss over last 100 batches: 0.24852 --- validation loss: 0.23426\n",
      "mean training loss over last 100 batches: 0.23344 --- validation loss: 0.22133\n",
      "mean training loss over last 100 batches: 0.22166 --- validation loss: 0.21089\n",
      "mean training loss over last 100 batches: 0.21147 --- validation loss: 0.20166\n",
      "mean training loss over last 100 batches: 0.20350 --- validation loss: 0.19482\n",
      "mean training loss over last 100 batches: 0.19786 --- validation loss: 0.18817\n",
      "------------------------- EPOCH 3 -------------------------\n",
      "mean training loss over last 100 batches: 0.19573 --- validation loss: 0.18604\n",
      "mean training loss over last 100 batches: 0.18998 --- validation loss: 0.18185\n",
      "mean training loss over last 100 batches: 0.18672 --- validation loss: 0.17884\n",
      "mean training loss over last 100 batches: 0.18449 --- validation loss: 0.17511\n",
      "mean training loss over last 100 batches: 0.17948 --- validation loss: 0.17151\n",
      "mean training loss over last 100 batches: 0.17675 --- validation loss: 0.16889\n",
      "------------------------- EPOCH 4 -------------------------\n",
      "mean training loss over last 100 batches: 0.17500 --- validation loss: 0.16811\n",
      "mean training loss over last 100 batches: 0.17374 --- validation loss: 0.16539\n",
      "mean training loss over last 100 batches: 0.17146 --- validation loss: 0.16371\n",
      "mean training loss over last 100 batches: 0.16949 --- validation loss: 0.16186\n",
      "mean training loss over last 100 batches: 0.16901 --- validation loss: 0.16040\n",
      "mean training loss over last 100 batches: 0.16636 --- validation loss: 0.15914\n",
      "------------------------- EPOCH 5 -------------------------\n",
      "mean training loss over last 100 batches: 0.16539 --- validation loss: 0.15940\n",
      "mean training loss over last 100 batches: 0.16498 --- validation loss: 0.15939\n",
      "mean training loss over last 100 batches: 0.16401 --- validation loss: 0.15651\n",
      "mean training loss over last 100 batches: 0.16339 --- validation loss: 0.15546\n",
      "mean training loss over last 100 batches: 0.16125 --- validation loss: 0.15455\n",
      "mean training loss over last 100 batches: 0.16148 --- validation loss: 0.15422\n",
      "------------------------- EPOCH 6 -------------------------\n",
      "mean training loss over last 100 batches: 0.16144 --- validation loss: 0.15337\n",
      "mean training loss over last 100 batches: 0.15951 --- validation loss: 0.15264\n",
      "mean training loss over last 100 batches: 0.15912 --- validation loss: 0.15150\n",
      "mean training loss over last 100 batches: 0.15865 --- validation loss: 0.15257\n",
      "mean training loss over last 100 batches: 0.15810 --- validation loss: 0.15110\n",
      "mean training loss over last 100 batches: 0.15709 --- validation loss: 0.15037\n",
      "------------------------- EPOCH 7 -------------------------\n",
      "mean training loss over last 100 batches: 0.15679 --- validation loss: 0.14980\n",
      "mean training loss over last 100 batches: 0.15480 --- validation loss: 0.14951\n",
      "mean training loss over last 100 batches: 0.15703 --- validation loss: 0.14846\n",
      "mean training loss over last 100 batches: 0.15533 --- validation loss: 0.14788\n",
      "mean training loss over last 100 batches: 0.15500 --- validation loss: 0.14813\n",
      "mean training loss over last 100 batches: 0.15315 --- validation loss: 0.14670\n",
      "------------------------- EPOCH 8 -------------------------\n",
      "mean training loss over last 100 batches: 0.15347 --- validation loss: 0.14680\n",
      "mean training loss over last 100 batches: 0.15310 --- validation loss: 0.14651\n",
      "mean training loss over last 100 batches: 0.15251 --- validation loss: 0.14620\n",
      "mean training loss over last 100 batches: 0.15162 --- validation loss: 0.14538\n",
      "mean training loss over last 100 batches: 0.15279 --- validation loss: 0.14484\n",
      "mean training loss over last 100 batches: 0.15240 --- validation loss: 0.14453\n",
      "------------------------- EPOCH 9 -------------------------\n",
      "mean training loss over last 100 batches: 0.15135 --- validation loss: 0.14479\n",
      "mean training loss over last 100 batches: 0.15080 --- validation loss: 0.14431\n",
      "mean training loss over last 100 batches: 0.15009 --- validation loss: 0.14503\n",
      "mean training loss over last 100 batches: 0.15079 --- validation loss: 0.14418\n",
      "mean training loss over last 100 batches: 0.14927 --- validation loss: 0.14333\n",
      "mean training loss over last 100 batches: 0.14974 --- validation loss: 0.14229\n",
      "------------------------- EPOCH 10 -------------------------\n",
      "mean training loss over last 100 batches: 0.14918 --- validation loss: 0.14315\n",
      "mean training loss over last 100 batches: 0.14971 --- validation loss: 0.14173\n",
      "mean training loss over last 100 batches: 0.14865 --- validation loss: 0.14227\n",
      "mean training loss over last 100 batches: 0.14877 --- validation loss: 0.14184\n",
      "mean training loss over last 100 batches: 0.14623 --- validation loss: 0.14136\n",
      "mean training loss over last 100 batches: 0.14643 --- validation loss: 0.14122\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "eval_every_n_batches = 100\n",
    "for epoch in range(10):\n",
    "    print(f'-'*25+f' EPOCH {epoch+1} '+f'-'*25)\n",
    "    for batch, (X, y, sequence_lenghts) in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits, _ = model(X, sequence_lenghts)\n",
    "        loss = criterion(logits, y)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch%eval_every_n_batches==0:\n",
    "            model.eval()\n",
    "            X, y, sequence_lenghts = next(iter(val_dataloader))\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits, _ = model(X, sequence_lenghts)\n",
    "            val_loss = criterion(logits, y)\n",
    "            print(f'mean training loss over last {eval_every_n_batches} batches: {np.mean(losses[-eval_every_n_batches:]):.5f} --- validation loss: {val_loss:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
